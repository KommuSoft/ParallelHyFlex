\section{Test-set}

De beschreven systemen in \chpref{parhyf} en \chpref{paradaphh} introduceren heel wat parameters. In de testen hebben volgende parameters laten vari\"eren:
\begin{enumerate}
 \item De grootte van het probleem: 100, 1'000 en $10'000$~variabelen;
 \item De moeilijkheid van het probleem: een \emph{expressie-variabele-ratio} van 4.26 of 42.6;
 \item De lokale invloed in de \emph{learning automaton} (in het \emph{relay hybridisation} gedeelte): $0.00,0.05,\ldots1.00$;
 \item De invloed van de globale waarden in de \emph{heuristic records} (in het \emph{ADHS} gedeelte): $0.00,0.05,\ldots1.00$;
 \item De grootte van de \emph{ervaring-set}: $1,2,\ldots,20$;
 \item Het aantal historische fitness-waardes in het \emph{AILLA}-gedeelte: $1,2,\ldots,20$
 \item Het aantal historische fitness-waardes in het \emph{AILLA}-gedeelte die op de lokale processor moeten gegenereerd zijn: $1,2,\ldots,10$;
 \item Het aantal beperkingen in de \emph{zoekruimte}: $1,2,\ldots,20$; en
 \item Het aantal processoren: $1,2,3,4$.
\end{enumerate}

Het overlopen van alle configuraties die men met deze parameters kan genereren is niet haalbaar. Zeker omdat er verschillende probleemgevallen moeten worden getest alvorens men besluiten uit de resultaten kan trekken. Voor elke probleem-grootte en moeilijkheid werd een set van $25$~instanties samengesteld. Deze set van problemen werd opgelost in simulaties met een toevallige gekozen configuratie. We zijn vooral ge\"interesseerd in de effecten wanneer het aantal processoren wordt opgedreven. Daarom is het aantal processoren geen parameter in de configuratie: al de andere parameters worden vastgezet en uitgevoerd op de verschillende aantallen aan processoren. Een simulatie stopt na $25$~seconden of eerder wanneer de oplossing gevonden wordt.

\paragraph{}
Deze techniek laat toe dat we de resultaten kunnen marginaliseren naar \'e\'en of meerdere variabelen onder de \emph{naive Bayes}-assumptie: er zijn geen afhankelijkheden tussen de verschillende vormen van data. In de realiteit verwachten we deze afhankelijkheden wel. Zolang de afhankelijkheden echter beperkt blijven\footnote{We verwachten bijvoorbeeld weinig invloed tussen parameters die geen betrekking hebben op hetzelfde component.}, kan men echter voorzichtige conclusies trekken. Bovenstaande techniek wordt vaak toegepast in een probleem met een groot aantal dimensies zoals bijvoorbeeld Monte-Carlo integratie.

\paragraph{}
In totaal werden $36'600$~simulaties gedraaid: $26'400$ op de gemakkelijke gevallen en $10'200$ op de moeilijke gevallen. Deze simulaties werden gedraaid op machines met een \emph{Intel i5 2400} processor ($4\times3.10~\mbox{GHz}$\footnote{Hoewel de processor vier verschillende \emph{cores} bezit, werd telkens slechts \'e\'en \emph{core} gebruikt.}, $6~\mbox{MiB}$ cache) met $3.7~\mbox{GiB}$ geheugen onder het \emph{Linux/Ubuntu 12.04 LTS} besturingssysteem.

\paragraph{}
Tijdens het draaien van een simulatie wordt per processor op de lokale machine een logbestand\footnote{Omdat de totale grootte van de logbestanden opliep tot meer dan 2~\mbox{TiB} werd deze niet ter beschikking gesteld.} aangemaakt. In zo'n logboek wordt regelmatig de toestand van de verschillende componenten genoteerd samen met het tijdstip. Met behulp van een reeks programma's geschreven in \emph{Perl} en \emph{Bourne Shell}\footnote{Deze programma's kunnen worden afgehaald op \url{http://goo.gl/3YNnp}.} wordt de relevante informatie gefilterd en samengevoegd in \'e\'en logboek per simulatie. Een simulatie gaat altijd gepaard met een initialisatie waarbij de verschillende processoren elkaar eerst contacteren. Bij meerdere processoren kan deze tijd gemakkelijk oplopen tot een halve seconde. Deze fase wordt uit de logboeken ge\"elimineerd: een simulatie begint op het moment dat een eerste oplossing in het geheugen wordt geladen. De andere reeks programma's marginaliseert vervolgens de simulaties: de logboeken van verschillende simulaties worden naast elkaar gelegd en per tijdstap wordt het gemiddelde, minimum, maximum, mediaan, eerste kwartiel, derde kwartiel en de standaardafwijking bepaald van de op dat moment beste oplossingen.

\paragraph{}
In plaats van de parameters van een component op zo in te stellen dat het component niet relevant is, kan men er ook voor opteren op het component uit te schakelen. Een probleem met dit principe is echter dat de rekenkracht die anders in dit component werd ge\"investeerd, nu in door de heuristieken zal worden gebruikt. Bij het daadwerkelijk oplossen van problemen kan dit natuurlijk een versnelling teweeg brengen, maar in het geval van onderzoek is het aangewezen deze componenten een passieve rol te laten spelen. Het \emph{ParHyFlex} systeem werd bovendien voornamelijk ontwikkelt voor het voeren van onderzoek. Door een groot aantal abstractieniveaus zijn de meeste componenten niet optimaal ge\"implementeerd. De rekenkracht die potentieel vrijkomt is bijgevolg een overschatting.

\paragraph{}
In de volgende subsecties zullen verschillende grafieken de evoluties van oplossingen voorstellen in de tijd. Elke grafiek doet dit volgens een bepaalde configuratie. Door verschillende simulaties te groeperen in \'e\'en grafiek en deze voor te stellen hopen we uitspraken te kunnen doen over een de volledige probleemruimte. Het gemiddelde is een metriek die gevoelig is voor extreme waarden. Het eerste en derde kwartiel worden bijgevolg ook op de grafieken vermeld als een lichte gevulde zone om meer informatie in verband met de verdeling te rapporteren.

\paragraph{}
De hypotheses die we naar voren schuiven omtrent de invloed van de parameters zijn niet wiskundig bewezen. Aan de hand van statistiek kan men wel een zekere significantie aantonen. Hiervoor werd volgende procedure ontwikkeld. Simulaties kunnen gerangschikt worden op basis van de kwaliteit van de beste oplossing wanneer de tijd op is of de tijd die verstreek alvorens men een oplossing vond. Vermits elke simulatie aan de parameters een bepaalde waarde toekent kunnen we deze waardes rangschikken wanneer we meerdere simulaties samennemen. Door het berekenen van de waarden die de drie sterkste en zwakste simulaties opleveren ontstaat een beeld welke parameters interessant zijn. Als nulhypothese stellen we dat elke waarde evenveel kans maakt om als sterk, zwak of middelmatig ge\"evalueerd te worden. We kunnen ook de kans berekenen dat de hypothese die naar voor geschoven wordt klopt. Enkel wanneer de kans dat de naar voor geschoven hypothese groter is dan 95\%, rapporteren we de hypothese.

%Wanneer componenten worden uitgeschakeld wordt de vrijgekomen rekenkracht besteed aan het verder ontwikkelen van de oplossingen. Door het uitschakelen van componenten is het bijgevolg niet mogelijk op een objectieve manier te oordelen over de invloeden van bepaalde parameters. Daarnaast is \emph{ParHyFlex} een systeem die voornamelijk ontwikkeld is voor het voeren van onderzoek. Het implementeren van componenten op een zeer abstract niveau impliceert echter \emph{overhead}. De meeste componenten zullen dan ook meer rekenkracht aanwenden dan strikt noodzakelijk is voor het vervullen van hun taken. Hierdoor kan men ook stellen dat de rekenkracht die we besparen met het uitschakelen van een component waarschijnlijk een overschatting is tegenover de potenti\"ele winst bij een praktische implementatie.