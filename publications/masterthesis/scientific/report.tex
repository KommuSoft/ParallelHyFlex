\documentclass{IEEEconf}
\title{Parallel Hyperheuristics}
\author{Willem Van Onsem\\promoter: prof. Bart Demoen}
\date{December 21, 2012}
\newcommand{\npc}{\textbf{NP-hard}}
\newcommand{\scal}{\mathcal{S}}
\newcommand{\pcal}{\mathcal{P}}
\newcommand{\ncal}{\mathcal{N}}
\newcommand{\func}[2]{\ensuremath{#1\left(#2\right)}}
\newcommand{\powerset}[1]{\func{\pcal{}}{#1}}
\begin{document}
\maketitle
\section{Problem description}
Most interesting optimization problems like the traveling salesman problem are proven to be \npc{}. Therefore the problem becomes infeasible if the input becomes large. One however can argue that the real solution to the problem is not that important. Building an algorithm that yields a solution approximating the real solution in a reasonable time is in most practical cases sufficient. A class of algorithms who are designed for this aim are metaheuristics.
\paragraph{Metaheuristics}
Metaheuristics are iterative algorithms with randomized aspects. They start from a solution for a specific problem who can be considered bad. Each iteration, the metaheuristic will try to improve the solution. To solve a problem effectively, one has to implement a metaheuristic with some domain specific knowledge. However the general strategies behind metaheuristics are quite domain independent: through the years general strategies have been developed like variable neighborhood search, genetic algorithms, tabu search,...
\paragraph{Solution convergence}
An advantage of metaheuristics is that if some conditions are met, the solution will converge to the optimal solution in time. Therefore one can set a certain time bound and let the algorithm optimize an initial solution. When the time bound is met, one can evaluate the solution. If the results are not satisfying, we can give the algorithm more time to optimize the solution. For some large problems however the convergence is too slow to yield an acceptable solution within a reasonable time interval.
\paragraph{Hyperheuristics}
Another problem is that some heuristics tend to get stuck in certain local optima. In order to deal with this problem, one can implement several heuristics who work on different principles. By implementing a range of heuristics, one has to make a decision which heuristic should be called at a certain point in time. A mechanism that makes such decisions and tweaks the heuristics (by changing it's parameters) is called a hyperheuristic. Hyperheuristics however don't solve the problem on their own: one has to implement an intelligent system that learns when one has to call a certain heuristic. Learning these conditions is non-trivial as well. Hyperheuristics are well known for their domain independent characteristics: if a wide range of low level heuristics is implemented a hyperheuristic can be used to find solutions for problems with different specifications. In theory this could imply that hyperheuristics can leave out the domain expert and work with general applicable low lever heuristics.
\paragraph{Parallelization}
To deal with large problems, one can try to parallelize these metaheuristics in order to yield better performance. This master thesis aims to be a in-depth study on how one can boost the performance of both metaheuristics and hyperheuristics by parallelizing their implementations. Since the problems occurring in sequential metaheuristics and hyperheuristics will probably also arise in their parallel counterparts, a study how implementation deal with such problems should deal with these problems is desirable.
\section{Literature Study}
\paragraph{Parallel Local Search}
The idea to parallelize metaheuristics is not new. Several successful attempts have already been made. \cite{verhoeven1996parallel} for instance gives an in-depth study how to parallelize local search algorithms. A local search algorithm is an algorithm that given an initial solution $s\in\scal{}$ and a neighborhood definition $\ncal{}:\scal{}\rightarrow\powerset{\scal{}}$, tries to improve the initial solution by iterating over the neighborhood of $s$: $\func{\ncal{}}{s}$. Each time a better solution is found, in the neighborhood definition, we continue our search by looking at the neighborhood of that new solution. We end when we have iterated over an entire neighborhood definition of a solution and failed to find a better solution.
\paragraph{Taxonomy}\cite{verhoeven1996parallel} furthermore defines some taxonomy for parallel local search algorithms: \emph{single-walk} and \emph{multiple-walk}. In a \emph{single-walk} algorithm we consider only one active solution at the time. However we use parallel computational power to speedup the search in the neighborhood of the active solution. In a \emph{multiple-walk} algorithm however each machine considers one active solution and tries to optimize that solution. The parallel algorithm stops when no walk can be further improved. A subset of the \emph{multiple-walk} algorithms are \emph{interactive multiple-walk} algorithms, where the processors can interact with each other while searching in order to boost performance. \emph{single-walk} algorithms are further divided into \emph{single-step} and \emph{multiple-step} algorithms. In a \emph{single-step} algorithm, the results are evaluated when the neighborhood of the active solution has finished. In a \emph{multiple-step} algorithm however, the processors will also inspect the neighborhood of the best solution in their part of the neighborhood of the initial active solution. The difference with a \emph{multiple-walk} algorithm however is that after some steps, the results are compared and the best solution becomes the new active solution. This taxonomy can be applied to most metaheuristics in general.
\paragraph{Theoretical speedup}
\cite{Shonkwiler94parallelspeed-up} gives a theoretical basis that \emph{independent multiple-walks} can be profitable and can even lead to a super linear speedup with the number of independent walks. This is done by representing the solution domain as a \emph{Markov random field} and calculate the expecting \emph{hitting time}: the time when we expect to reach the global solution. This however has nothing to do with parallel implementations of these walks since we can simply simulate multiple walks on a sequential machine advancing each independent walk sequentially.
\paragraph{Empirical super linear speedup}
\cite{Alba05} summarizes research on parallel metaheuristics into a book. The book reports some empirical evidence of algorithms who reached super linear speedup. However in most cases this super linear speedup is reached due to larger memories and caches and due to implementation issues: since the parallelized algorithms have to deal with smaller amounts of data, they have faster access to that data and thus reduce the overall time. In the rest of the book most algorithms report a near-linear speedup. This is in most cases due to communication overhead and accidental redundant calculations.
\paragraph{Parallel implementation problems}
An important part of \cite{Alba05} deals with problems that arise when parallelizing metaheuristics. One problem is that two instances of the algorithm could find the same solution and the evolution of these solutions is shared. In this case, two or more processors wouldn't yield any speedup since they are working on the same results. This is solved by using random number generators who are seeded differently. This system doesn't guarantee two machines won't yield the same results, however the communication overhead to deal with this problem is considered worse than the delay caused by this potential conflict. Another aspect is load balance. Most metaheuristics work with a number of iterations before results are synchronized. In many cases however, incrementing the number of iterations happens in a probabilistic fashion. Therefore it is possible that a processor reaches the number of iterations earlier than the other processors. A solution could be that idling processors resume computation by initializing an additional walk originating somewhere from their historical calculations.
\paragraph{Taxonomy for parallel metaheuristics}
An important chapter of the \emph{Handbook on Metaheuristics}\cite{gendreau2010handbook} is \emph{chapter 17: parallel metaheuristics}\cite{craig}. This chapter gives a state-of-the-art survey where the different sources of parallelization are identified. Besides the aspects mentioned in \cite{verhoeven1996parallel} which can be summarized as \emph{multi-search}, \emph{domain decomposition} is introduced as a form of parallelization where each processors searches a fixed subset of the solution space. The chapter states that \emph{domain decomposition} can force two parallel systems not to search in the same solution space while \emph{multi-search} merely tries to avoid this (without guarantees). The chapter furthermore uses a different taxonomy which consists out of three dimensions. The first dimension is \emph{Search Control Cardinality} (\emph{1 Process Control} or \emph{$p$ Process Control}). The second dimension is the \emph{Communication Mode} (\emph{Synchronous} or \emph{Asynchronous}) and is further divided by the quantity of information (\emph{Rigid}, \emph{Knowledge synchronization}, \emph{Collegial} and \emph{Knowledge collegial}). The final dimension deals with \emph{Search Differentiation} (\emph{Same Initial Point/Different Strategies}, \emph{Multiple Initial Points/Different Strategies}, \emph{Same Initial Point/Same Strategies} and \emph{Multiple Initial Points/Same Strategies}). A distinction is also made between \emph{Neighborhood-based} metaheuristics and \emph{Population-based} metaheuristics. The different elements in the taxonomy are evaluated based on potential gain by parallel implementations. For instance it is stated that in general \emph{$p$ Process Control} algorithms yield better results than a centralized implementation. The reason is that these algorithms are more integrated enabling to share while the search is in progress. The downside however is the communication overhead created by these algorithms. Issues specific for communication on \emph{$p$ Process Control} implementations are documented and partly solved in \cite{Toulouse95communicationissues}. \cite{gendreau2010handbook} points out that sharing best solutions with all the processes will lead to premature convergence. Therefore in most cases random selection procedures are implemented selecting a solution to share with a group of processes.
\paragraph{Speedup factor as a good metric?}
Another aspect of the chapter is the meaning of the \emph{speedup} factor in parallel metaheuristics. The chapter claims that since in most cases we don't know the exact solution or the metaheuristics are aborted before they reach the exact solution, \emph{speedup} isn't the correct factor to evaluate parallel metaheuristics against their sequential counterparts. Therefore the author claims that a good parallel metaheuristic should outperform his sequential counterpart by providing better quality solutions, a more robust system, or by computational complexity: the overall computational effort of the parallel algorithms is lower than the computational effort of the sequential algorithm.
\paragraph{HyFlex}
Since hyperheuristics aim to be problem independent, we cannot simply apply conclusions drawn for metaheuristics on hyperheuristics. Furthermore it's hard to give evidence that some hyperheuristic outperforms other hyperheuristics since we can only test on specific domains. In order to achieve more insights into hyperheuristics, \cite{conf/cec/BurkeCHKOPRG10} proposed a framework called \emph{HyFlex} to compare the results of hyperheuristics in a rather domain independent fashion. In 2011, a competition called \emph{Cross-domain Heuristic Search Challenge (CHeSC2011)}\cite{Burke:2011:CHS:2177360.2177415} was held in order to compare several implementation of hyperheuristics. An interesting aspect is why some hyperheuristics perform better than others.
\paragraph{PHunter}
An interesting and quite new way to implement a hyperheuristic is \emph{PHunter}\cite{conf/lion/ChanXIC12}. The proposed algorithm is based on pearl hunting and achieved fourth place on \emph{CHeSC2011}. In a first stage of the algorithm, a pool of solutions in created. Some basic perturbation heuristics are applied to that pool in order to generate different solutions. Since we are working with a population, we can also analyze the solution surface and thus for instance check if some instances are trapped around a ``\emph{buoy}''. A \emph{buoy} is a solution we want to avoid, like for instance our old solutions. In a second phase some local search is performed on the population. However we don't search deep in order to save time. In the final phase of the algorithm, we only consider promising results and perform deep local searches. An interesting aspect of \emph{PHunter} is that concurrently, the algorithm tries to learn about it's environment. For instance when a lot of related solutions don't yield a better result after applying deep local search, the local search is 
eliminated for the rest of the solutions in order to save time. By tweaking the search based on characteristics of the surface, a more efficient optimization can be achieved.
\paragraph{The effect of low level heuristics}
Another interesting paper as a result of the \emph{CHeSC2011} competition is \cite{Misir:2012:ESL:2402710.2402754}. This paper tries to verify how performance of a hyperheuristic changes when the set of metaheuristics is changed. The papers shows that the performance of a hyperheuristic highly depends on the distribution of the nature of the metaheuristics together with the set size and the speed of the metaheuristics. Therefore 11 metaheuristics were implemented. With a parameter, one can slow down the performance of a low level heuristic artificially. Furthermore different selection criteria were used to a select a metaheuristic. The best selection criteria strongly differed when different sets were used. Since a hyperheuristic aims to be somehow domain-independent, one should strive to robust selection mechanisms who don't suffer from small tweaks on the heuristics set.
\paragraph{Parallel hyperheuristics}
Some attempts have been made to parallelize hyperheuristics. \cite{journals/memetic/SeguraML11} aims to implement a parallel hyperheuristic in order to assign a frequency for a communication channel in a \emph{GSM} network. An island-based parallel implementation is used in order to solve the problem. The implementation doesn't achieve linear speedup but produces better quality results and leads to faster convergence towards the real solution.
\section{Approach}
\paragraph{Comparative study}
In order to implement widely applicable parallel hyperheuristics, a comparative study on the different implementations who were submitted for the \emph{CHeSC2011} competition could give more insights. The focus on this comparative study should be what aspects of hyperheuristics contribute to a good hyperheuristic and which aspects can result in bad performance.
\paragraph{Generic framework}
In a second phase, a generic framework should be built. This generic framework will be based on the implementations of the \emph{CHeSC2011} competition where concepts are generalized and algorithms are subdivided into replaceable parts. We think implementing such framework in a functional fashion will be the most effective way: it enables the user to replace a certain concept of a certain search strategy with another one in order to perform research on the influence this concept has. The generic framework will be based on the implementations of \emph{CHeSC2011}, however implementation details should be left out: concepts will be simplified in order to measure the performance of the aspects instead of for instance the constants they are carrying.
\paragraph{Parallel HyFlex}
The current \emph{HyFlex} framework is not parallelized. By parallelized we mean two processes can share solutions with each other, or perform low level heuristics in a parallel way. A logical step in order to parallelize hyperheuristics is to parallelize \emph{HyFlex}. Therefore we could use the \emph{Message Passing Interface (MPI)} in order to exchange solutions, do global parameter tweaking and perform the low level heuristics in a parallel fashion. An important aspect is communication overhead. Since their exist some successful metaheuristics implementations who work almost independent from each other, we think it's arguable that implementations with limited communication can yield good results too. Therefore we will implement this \emph{parallel HyFlex} framework with a \emph{communication threshold}. This threshold specifies the magnitude of data to be shared and enables research on the effect of more communication on the overall performance of hyperheuristics.
\paragraph{Parallel generic framework}
When the \emph{parallelized HyFlex} framework enables one to exchange solutions between several processors, we can implement the parallelized counterparts of the concepts implemented in the sequential generic framework. We aim to parallelize these strategies in a transparent way, where some aspects like for instance an acceptance function don't have to be adapted to the parallel context. A second possible advantage of transparent implementations is that developers could write their own hyperheuristics in a sequential fashion without knowing these algorithms are parallelized. Since the parallel implementation will be rather similar to the sequential implementations, one cannot only compare results between the sequential and parallel counterparts, but also look for differences in what makes a hyperheuristic a successful one: their is a chance that algorithms who yield great results in a sequential fashion are outperformed by parallel implementations of inferior sequential hyperheuristics.
\paragraph{Prove of insights}
A final task aiming to gain more insights in (parallel) hyperheuristics, is to model these hyperheuristics mathematically in order to prove the function of certain aspects in sequential and parallel hyperheuristics. As a template, we think modeling the problems like \emph{stationary} and \emph{non-stationary Markov chains} like in \cite{Shonkwiler94parallelspeed-up} could yield mathematical evidence.
\bibliographystyle{ieeetr}
\bibliography{biblio}
\end{document}