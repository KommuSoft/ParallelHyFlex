\section{\emph{AVEG-Nep: Reinforcement Learning Approach} (\#16)}
\seclab{aveg-nep}

\subsection{Implementatie}
\emph{AVEG-Nep}\cite{chesc-aveg-nep} is werkt op basis \emph{Reinforcement learning}\cite{rlaiacaml}. In dit systeem moeten vijf componenten worden gedefinieerd:
\begin{enumerate}
 \item Een \emph{set acties}: een actie is een type \abllhn{} (\abmt{}, \abls{}, \abco{}, \abrr{}) verreikt met de waarde van de relevante parameters (\emph{depth of search} of \emph{intensity of mutation}) gekwantiseerd per interval van 0.2. Wanneer we een bepaalde actie kiezen, kiezen we uniform een \abllh{} die tot het geselecteerde type behoort en voeren deze uit met de geselecteerde parameters.
 \item Een \emph{beloningsfunctie} ofwel ``\emph{reward function}'': het verschil in fitnesswaarde tussen de nieuwe oplossing en de oorspronkelijke oplossing gedeeld door de tijd die de heuristiek nodig had om de nieuwe oplossing te berekenen.
 \item Een \emph{toestandsvoorstelling}: hier is dit het gewogen gemiddelde van het verschil in fitness-waarde over de tijd voor (we berekenen dit via ``\emph{exponential smoothing}''\cite{Taylor2003a})
 \item Een \emph{beleid} ofwel ``\emph{policy}'': hiervoor gebruiken we de klassieke ``\emph{$\epsilon$-greedy}''-methode.
 \item Een \emph{Leerfunctie} ofwel ``\emph{learning function}'': ook hiervoor gebruiken we bij \emph{AVEG-Nep} een ``\emph{exponential smoothing}''-functie.
\end{enumerate}
\emph{AVEG-Nep} laat vier verschillende agenten tegelijk werken op de het probleem. Men heeft immers een tweede oplossing nodig bij het toepassen van een \abco{} \abh{}. In dat geval kiest men de oplossing van uniform een andere agent om een \abco{} mee te realiseren.

\subsection{Kritiek}
\begin{itemize}
 \item Geen onderscheid tussen \abhn{} van hetzelfde type: parameters kunnen een andere schaal hebben doorheen de verschillende \abhn{}.
 \item Binnen eenzelfde type kunnen sommige \abhn{} beter presteren dan anderen: weinig motivatie voor deze vorm van \emph{domain relaxation}.
 \item Problemen met de voorstelling van de toestand: naarmate het algoritme vordert verwachten we een minder sterke groei van de fitness-functie. Het algoritme kan later terechtkomen in toestanden die nog onvoldoende verkend zijn.
\end{itemize}