\section{Parallelle algoritmen}

\subsubsection{Motivering}

De ``Wet van Moore''\cite{4785860} stelt dat het aantal transistors op een chip elke achttien maanden verdubbelt. De klokfrequentie stagneert echter. \cite{nomoore} verklaart dit door te stellen de gemiddelde afstand van een geleider slechts met $1/\sqrt{2}$ afneemt. Vermits de snelheid van het kloksignaal beperkt is tot de overdrachtssnelheid van een geleider, stelt men dat de kloksnelheid niet evenredig verhoogt kan worden. Het gevolg van deze evolutie is dat complexe problemen enkel uitgerekend kunnen worden indien men voldoende rekentijd beschikbaar stelt.
\paragraph{}
Een reactie is de introductie van het ``\emph{parallel rekenen}'' ofwel ``\emph{parallel computing}''. Een probleem wordt opgelost door programma's die op verschillende processoren tegelijk te draaien en elk een deel van het probleem oplossen. Door berichten uit te wisselen tussen de verschillende processoren of door een gemeenschappelijk geheugen aan te bieden communiceren de programma's die op de verschillende processoren draaien met elkaar.\cite{books/bc/KumarGGK94}

\paragraph{}
Door met verschillende processoren te werken hoopt men de rekentijd drastisch naar beneden te kunnen halen. Dit is echter niet de enige motivatie: doorgaans verbruiken zogenaamde \emph{multi-core} processoren - processoren waarbij verschillende \emph{kernen} op dezelfde machine aanwezig zijn - ook minder energie. Dit komt bijvoorbeeld omdat andere hardware zoals het moederbord slechts \'e\'enmaal voorkomen in de configuratie. Men kan dus ook beogen de aanwezige hardware effici\"enter te gebruiken.\cite{Korthikanti:2009:APA:1678990.1679696,parenergy}

\paragraph{}
Sommige problemen kunnen bovendien enkel opgelost worden met parallelle algoritmen: geografische simulaties houden variabelen bij voor een groot aantal locaties op aarde. Deze variabelen leiden tot het gebruiken van grote hoeveelheden geheugen, en passen meestal niet op \'e\'en machine. Door elke machine een deel van de data te laten opslaan en manipuleren kan dit probleem worden opgelost.\cite{journals/pc/HawickCJ03}

\paragraph{}
Parallelle algoritmen worden ook ingeschakeld wanneer de data gevoelig is en niet zomaar kan worden uitgewisseld: indien verschillende bedrijven een globale planning willen opstellen, maar geen details over de interne werking openbaar wensen te maken, kunnen parallelle algoritmen een oplossing bieden\cite{Gaspero_amultiagent}.

\subsubsection{Prestaties van parallelle algoritmen}

De prestaties van een parallel algoritme worden meestal gemeten op basis van \emph{speed-up}: de mate waarin we virtueel de kloksnelheid kunnen opdrijven. We defini\"eren eerst enkele concepten waarna we kort een taxonomie hieromtrent toelichten.

\begin{definition}[Wall time, Speed-up, Effici\"entie\cite{books/bc/KumarGGK94,Alba2005book}]
We defini\"eren de \emph{wall time} $T$ als de tijd tussen de start van het algoritme en het moment waarop het algoritme stopt. De \emph{wall time} maakt abstractie van het aantal processoren. De \emph{speed-up} is de verhouding tussen de \emph{wall time} bij \'e\'en processor $T_1$ en de \emph{wall time} bij $p$ processoren $T_p$:
\begin{equation}
\funm{speed-up}{p}=\displaystyle\frac{T_1}{T_p}
\end{equation}
Indien de algoritmen stochastisch van aard zijn, wordt de verhoudingen tussen de gemiddelde \emph{wall time} gebruikt.
Wanneer we de \emph{speed-up} delen door het aantal processoren $p$ spreken we over de \emph{effici\"entie}:
\begin{equation}
\funm{effici\"entie}{p}=\funm{speed-up}{p}/p
\end{equation}
Andere parameters om parallelle algoritmen te evalueren zijn: \emph{totale parallelle overhead} en \emph{processor-tijd product}\cite{books/bc/KumarGGK94}.
\end{definition}

\paragraph{}
Men streeft ernaar dat de \emph{speed-up} in het geval van $p$~processoren gelijk is aan~$p$. Dit fenomeen noemt men \emph{lineaire speed-up}. Wanneer men voor een ineffici\"ent algoritme een parallel equivalent schrijft, stelt men vaak een lineaire \emph{speed-up} vast. Dit komt omdat een na\"ief algoritme meestal geen relaties tussen de verschillende componenten in de invoer zal uitbuiten. Het maakt bijgevolg niet uit of de data door \'e\'en of meerdere processoren wordt verwerkt. In de meeste gevallen kan men wel degelijk relaties in de aanwezige data uitbuiten en is het minder evident om lineaire \emph{speed-up} te verwezenlijken.

\paragraph{}
Omdat de definitie van \emph{speed-up} toelaat om voor zwakke algoritmen een sterke \emph{speed-up} te realiseren, werd het concept \emph{strong speed-up} ge\"introduceerd. In het geval van \emph{strong speed-up} wordt de snelste parallelle implementatie tegenover de sterkste sequenti\"ele implementatie geplaatst. Omdat algoritmen in een parallelle context meestal maar over een beperkt deel van de data beschikken, wordt het ontwikkelen van \emph{lineaire speed-up} bijgevolg complexer en in sommige gevallen zelfs onmogelijk.\cite{Alba2005book}

\paragraph{}
Een paper uit 1986\cite{journals/pc/FaberLW86} stelt dat men nooit een \emph{speed-up} groter dan $p$ kan realiseren. Een parallel algoritme kan immers ge\"emuleerd worden door \'e\'en processor die vervlochten de instructies van de verschillende processoren uitvoert. In dat geval dient deze processor hoogstens $p$ keer het aantal instructies van \'e\'en parallelle processor uit te voeren. In eerder uitzonderlijke omstandigheden stelt men wel degelijk \emph{superlineaire speed-up} vast. Dit is te wijten aan het effici\"enter gebruik van de aanwezig hardware. Cache is hierbij een uitstekend voorbeeld: bij multi-core processoren hebben de verschillende ``\emph{kernen}'' ofwel ``\emph{cores}'' meestal een eigen segment aan cache. Sommige gegevens worden gebruikt door alle processoren, bijgevolg wordt deze data in de gemeenschappelijke cache opgeslagen. Een gevolg is dat er meer ruimte beschikbaar is in de aparte caches voor specifieke data. Men noemt het effect van superlineaire versnellingen dan ook meestal het ``\emph{cache-effect}''\cite{cacheEffect,superlineairspeedup,journals/pc/Janssen87,journals/pc/Parkinson86}.

\subsubsection{Beperkingen en oplossingen}

Het verhogen van de snelheid met behulp van parallelle algoritmen is meestal een feit. Het optekenen van \emph{lineaire speed-up} is echter geen evidentie. Dit komt onder meer door de ``\emph{Wet van Amadahl}''\cite{Amdahl:1967:VSP:1465482.1465560}. De meeste algoritmen hebben een zekere nood aan het sequentieel uitvoeren van bepaalde instructies. De Wet van Amadahl stelt dat gegeven de fractie $f_{\smbox{seq.}}$ van het aantal verplicht sequenti\"ele instructies, er een plafond bestaat voor de maximale \emph{speed-up} en dat men met een arbitrair aantal processoren:
\begin{equation}
\mbox{speed-up}<\displaystyle\frac{1}{f_{\mbox{seq.}}}
\end{equation}


\paragraph{}
Meestal zullen we het aantal processoren enkel verhogen wanneer de omvang van het probleem groter wordt. De ``\emph{Wet van Gustafson}''\cite{Gustafson:1988:RAL:42411.42415} stelt dat wanneer de niet te parallelliseren fractie $f_{\mbox{seq.}}$ niet sneller dan \bigoh{\log n} schaalt bij een probleemgrootte van $n$, \emph{lineaire speed-up} gehaald kan worden. Vermits we vooral ge\"interesseerd in het parallelliseren van grote optimalisatieproblemen, behoort \emph{lineaire speed-up} alsnog tot de mogelijkheden.

\subsection{De belangrijkste parallellisatie paradigma's}

Meestal beschouwd men twee verschillende modellen met betrekking tot parallellisatie: \emph{Message Passing Interface (MPI)}, \emph{Shared-Adress-Space}. \cite{books/daglib/0066939} beschrijft ook een derde paradigma: \emph{Live Datastructures}. Dit paradigma sluit nauw aan bij \emph{Shared-Adress-Space} en wordt daarom niet beschouwd. We geven hieronder een korte samenvatting.\cite{books/bc/KumarGGK94,books/daglib/0066939}

\subsubsection{Message Passing Interface}

In het geval van \emph{MPI} beschouwen we een set \emph{agents}: processen die onafhankelijk werken op een eigen stuk geheugen. Agenten kunnen met elkaar samenwerken door berichten uit te wisselen over een netwerk. Men kan dit paradigma ook implementeren op systemen waarbij de agenten op de verschillende \emph{cores} van eenzelfde machine draaien: in dat geval is het de taak van het besturingssysteem om de berichten af te leveren bij het relevante proces.\cite{books/bc/KumarGGK94,books/daglib/0066939,books/daglib/0015079,mpi10}

\paragraph{}
Indien de \emph{agents} op verschillende machines draaien, werken ze meestal volgens een bepaalde \emph{topologie}: een structuur die bepaalt welke machines met elkaar verbonden zijn. \emph{MPI} heeft een sterke focus op \emph{group communication}: het uitwisselen van data met een significant deel van de agenten tegelijk. In plaats van in dit geval alle agenten een bericht te sturen kan men afhankelijk van de topologie het aantal berichten reduceren.\cite{books/bc/KumarGGK94}

\subsubsection{Shared-Adress-Space}

In het geval van \emph{Shared-Adress-Space} beschouwd men een geheugen die toegankelijk is voor alle processoren. Elke processor kan data bijgevolg deze data uitlezen en manipuleren. Bij voorkeur beschouwen we \'e\'en machine met \'e\'en fysisch geheugen en verschillende processoren. Wanneer er sprake is van meerdere processoren wordt het gedeelde geheugen ge\"emuleerd door de data over een netwerk te sturen. Om te voorkomen dat men voor rekenintensieve taken te veel data over het netwerk stuurt, zal men meestal een onderscheid maken tussen het lokaal en vreemd geheugen.\cite{books/bc/KumarGGK94}

\paragraph{}
\emph{Tuple Space} is een vorm van \emph{Shared-Adress-Space} met een globaal geheugen: de \emph{tuple space}. Elke agent kan tuples toevoegen in de tuple space en een query op de ruimte uitvoeren. Door tuples toe te voegen die vervolgens door andere agenten worden verwerkt, ontstaat er een communicatieprincipe. Een \emph{tuple space} verschilt echter van een \emph{message passing interface} omdat men meer abstractie maakt van de bestemming van een boodschap. Bekende implementaties van een \emph{Tuple Space} zijn \emph{Linda} en \emph{JavaSpaces}.\cite{books/bc/KumarGGK94,books/daglib/0066939,books/daglib/0015079,Gelernter85,javaspaces}