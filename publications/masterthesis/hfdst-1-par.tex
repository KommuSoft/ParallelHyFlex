\section{Parallelle algoritmes}

\subsubsection{Motivatie}

De ``Wet van Moore''\cite{4785860} stelt dat het aantal transistors verdubbelt elke achttien maanden. De klokfrequentie stagneert echter. \cite{nomoore} verklaart dit door te stellen de gemiddelde afstand van een geleider slechts met $1/\sqrt{2}$ afneemt, vermits het kloksignaal beperkt is tot de overdrachtssnelheid van een geleider, leidt men hieruit af dat de kloksnelheid niet evenredig verhoogt. Het gevolg van deze evolutie is dat complexe problemen enkel uitgerekend kunnen worden indien men voldoende rekentijd beschikbaar stelt.
\paragraph{}
Een reactie is de introductie van het ``\emph{parallel rekenen}'' ofwel ``\emph{parallel computing}''. Een probleem wordt opgelost door programma's die op verschillende processoren tegelijk te draaien en elk een deel van het probleem oplossen. Door berichten uit te wisselen tussen de verschillende processoren of door een gemeenschappelijk geheugen aan te bieden communiceren de programma's die op de verschillende processoren draaien met elkaar.\cite{books/bc/KumarGGK94}

\paragraph{}
Door met verschillende processoren te werken hoopt men de rekentijd drastisch naar beneden te kunnen halen. Dit is echter niet de enige motivatie: doorgaans verbruiken zogenaamde \emph{multi-core} processoren - processoren waarbij verschillende \emph{kernen} op dezelfde machine aanwezig zijn - ook minder energie. Dit komt bijvoorbeeld omdat andere hardware zoals het moederbord slechts \'e\'enmaal voorkomen in de configuratie. Men kan dus ook beogen de aanwezige hardware effici\"enter te gebruiken.\cite{energyEfficency}

\paragraph{}
Sommige problemen kunnen bovendien enkel opgelost worden met parallelle algoritmen: geografische simulaties houden variabelen bij voor heel wat locaties op de aarde. Deze variabelen leiden tot het gebruiken van grote hoeveelheden geheugen, meestal kan men niet al de data in het geheugen van \'e\'en machine opslaan. Door elke machine een deel van de data te laten opslaan en manipuleren kan dit probleem worden opgelost.\cite{journals/pc/HawickCJ03}

\paragraph{}
Parallelle algoritmes worden ook ingeschakeld wanneer de data gevoelig is en niet zomaar kan worden uitgewisseld: indien verschillende bedrijven een globale planning willen opstellen, maar geen details over de interne werking openbaar wensen te maken, kunnen parallelle algoritmes een oplossing bieden\cite{Gaspero_amultiagent}.

\subsubsection{Prestaties van parallelle algoritmen}

De prestaties van een parallel algoritme worden meestal gemeten op basis van \emph{speed-up}: de mate waarin we virtueel de kloksnelheid kunnen opdrijven. We defini\"eren eerst enkele concepten waarna we kort een taxonomie hieromtrent toelichten.

\begin{definition}[Wall time, Speed-up, Effici\"entie\cite{books/bc/KumarGGK94,Alba2005book}]
We defini\"eren de \emph{wall time} $T$ als de tijd tussen de start van het algoritme en het moment waarop het algoritme stopt. De \emph{wall time} maakt abstractie van het aantal processoren. De \emph{speed-up} is de verhouding tussen de \emph{wall-time} bij \'e\'en processor $T_1$ en de \emph{wall-time} bij $p$ processoren $T_p$:
\begin{equation}
\funm{speed-up}{p}=\displaystyle\frac{T_1}{T_p}
\end{equation}
Indien de algoritmen stochastisch van aard zijn, wordt de verhoudingen tussen de gemiddelde \emph{wall-time} gebruikt.
Wanneer we de \emph{speed-up} delen door het aantal processoren $p$ spreken we over de \emph{effici\"entie}:
\begin{equation}
\funm{effici\"entie}{p}=\funm{speed-up}{p}/p
\end{equation}
Andere parameters om parallelle algoritmen te evalueren zijn: \emph{totale parallelle overhead} en \emph{processor-tijd product}\cite{books/bc/KumarGGK94}.
\end{definition}

\paragraph{}
Men streeft ernaar dat de \emph{speed-up} in het geval van $p$~processoren gelijk is aan~$p$. Dit fenomeen noemt men \emph{lineaire speed-up}. Wanneer men voor een ineffici\"ent algoritme een parallel equivalent schrijft, stelt men vaak een lineaire \emph{speed-up} vast. Dit komt meestal omdat een na\"ief algoritme meestal geen relaties tussen de verschillende componenten in de invoer zal uitbuiten. Het niet uit of de data door \'e\'en of meerdere processoren wordt verwerkt. In de meeste gevallen kan men wel degelijk relaties in de aanwezige data uitbuiten en is het minder evident om lineaire \emph{speed-up} eerder optimistisch.

\paragraph{}
Omdat de definitie van \emph{speed-up} toelaat om voor zwakke algoritmen een sterke \emph{speed-up} te realiseren, werd \emph{strong speed-up}. In het geval van \emph{strong speed-up} wordt de snelste parallelle implementatie tegenover de sterkste sequenti\"ele implementatie geplaatst. Omdat algoritmes in een parallelle context meestal maar een beperkt deel van de data ter beschikking hebben, wordt het ontwikkelen van \emph{lineaire speed-up} bijgevolg complexer en in sommige gevallen zelfs onmogelijk.\cite{Alba2005book}

\paragraph{}
Een paper uit 1986\cite{journals/pc/FaberLW86} stelt dat men nooit een \emph{speed-up} groter dan $p$ kan realiseren. Een parallel algoritme kan immers ge\"emuleerd worden door \'e\'en processor die vervlochten de instructies van de verschillende processoren uitvoert. In dat geval dient deze processor hoogstens $p$ keer het aantal instructies van \'e\'en parallelle processor uit te voeren. In eerder uitzonderlijke omstandigheden stelt men echter wel degelijk \emph{superlineaire speed-up} vast. Dit is te wijten aan het effici\"enter gebruik van de aanwezig hardware. Cache is hierbij een uitstekend voorbeeld: bij multi-core processoren hebben de verschillende ``\emph{kernen}'' ofwel ``\emph{cores}'' meestal een eigen segment aan cache. Sommige gegevens worden gebruikt door alle processoren, bijgevolg wordt deze data in de gemeenschappelijke cache opgeslagen. Een gevolg is dat er meer ruimte beschikbaar is in de aparte caches voor specifieke data. Men noemt het effect van superlineaire versnellingen dan ook meestal het ``\emph{cache-effect}''\cite{cacheEffect,superlineairspeedup,journals/pc/Janssen87,journals/pc/Parkinson86}.

\subsubsection{Beperkingen en oplossingen}

Het verhogen van de snelheid met behulp van parallelle algoritmen is meestal een feit. Het optekenen van \emph{lineaire speed-up} is echter geen evidentie. Dit komt onder meer door de ``\emph{Wet van Amadahl}''\cite{Amdahl:1967:VSP:1465482.1465560}. De meeste algoritmes hebben een zekere nood aan het sequentieel uitvoeren van bepaalde instructies. De Wet van Amadahl stelt dat gegeven de fractie $f_{\smbox{seq.}}$ van het aantal verplicht sequenti\"ele instructies, er een plafond bestaat voor de maximale \emph{speed-up} en dat men met een arbitrair aantal processoren:
\begin{equation}
\mbox{speed-up}<\displaystyle\frac{1}{f_{\mbox{seq.}}}
\end{equation}


\paragraph{}
Meestal zullen we het aantal processoren echter enkel verhogen wanneer het computationele probleem groter wordt. De ``\emph{Wet van Gustafson}''\cite{Gustafson:1988:RAL:42411.42415} stelt dat wanneer de niet te parallelliseren fractie $f_{\mbox{seq.}}$ niet sneller dan \bigoh{\log n} meeschaalt, met $n$ de probleemgrootte, \emph{lineaire speed-up} gehaald kan worden. Vermits we vooral ge\"interesseerd in het parallelliseren van grote optimalisatieproblemen, behoort \emph{lineaire speed-up} misschien alsnog tot de mogelijkheden.

\subsection{De belangrijkste parallellisatie paradigma's}

Meestal beschouwd men twee verschillende modellen met betrekking tot parallellisatie: \emph{Message Passing Interface (MPI)} en \emph{Tuple Space}. We geven hieronder een korte samenvatting.

\subsubsection{Message Passing Interface}

In het geval van \emph{MPI} beschouwen we een set \emph{agents}: processoren die onafhankelijk werken op een eigen stuk geheugen. Deze agenten interagerend door het uitwisselen van berichten over een netwerk. Dit hoeft niet te betekenen dat de agenten ook effectief draaien op verschillende machines: men kan op elke \emph{core} van een machine een onafhankelijke agent laten werken en vervolgens de boodschappen laten bezorgen met behulp van de faciliteiten die de meeste besturingssystemen aanbieden.

\paragraph{}
In het geval de \emph{agents} wel degelijk op verschillende machines draaien werken ze meestal volgens een bepaalde \emph{topologie}: een structuur die bepaalt welke machines met elkaar verbonden zijn. Een belangrijk aspect in \emph{MPI} is ``\emph{Group Communication}'': agenten die met een significant gedeelte van de andere agenten tegelijk communiceren. Voor elke topologie zijn er dan ook algoritmes uitgewerkt om het aantal berichten tot een minimum te beperken.

\subsubsection{Tuple Space}

Een \emph{Tuple Space} is een model waarbij agenten kennis hebben van een globale wereld: de \emph{Tuple Space}. Elke agent kan tuples toevoegen in de tuple space en een query op de ruimte uitvoeren. Door tuples toe te voegen die dan door andere agenten worden verwerkt ontstaat er een communicatieprincipe. Een \emph{tuple space} verschilt echter van een \emph{message passing interface} omdat men meer abstractie maakt van de bestemming van een boodschap.