\section{RH: Relay Hybridisation}

Naast \emph{ADHS} is \emph{RH} ook een mechanisme om heuristieken te selecteren. Per iteratie in de fase zal met stijgende kans dit mechanisme geactiveerd worden. Op basis van een \emph{learning automaton}\cite{RePEc:cla:levarc:481} wordt een heuristiek geselecteerd die wordt toegepast op de actieve oplossing. Elke heuristiek onderhoudt een lijst met heuristieken die effectief bleken als tweede transitiefunctie. Duplicaten in de lijst zijn toegelaten en waardoor een vorm van kansen ontstaat. Op basis van de lijst wordt vervolgens in $0.25\%$ van de gevallen een heuristiek geselecteerd. In het andere geval wordt er een toevallige heuristiek geselecteerd die eventueel niet in de lijst voorkomt. De tweede geselecteerde heuristiek wordt vervolgens toegepast op het resultaat van de eerste heuristiek. De \emph{learning automaton} gebruik een \emph{lineair reward-interaction update schema} waardoor combinaties die globaal betere oplossingen vinden meer kans maken in de volgende iteraties. Enkel wanneer er een globaal betere oplossing wordt gevonden, wordt deze actie beloond in de \emph{learning automaton}.

\paragraph{}
In het geval van een \emph{learning automaton}, is \emph{mimetism} een populaire oplossing: het nabootsen van de toestanden van de andere processen. Na elke fases stuurt het proces de kansvector door naar de andere processoren. Vervolgens worden de verschillen gedeeltelijk doorgerekend waardoor de verschillende kansvectoren dichter bij elkaar komen te liggen. Door de wijzigingen slechts gedeeltelijk door te rekenen, behouden de verschillende processoren een eigen kansvector en is het minder waarschijnlijk dat de kansvector teveel naar een beperkte set van heuristiek schuift.