\section{Optimalisatieproblemen}

We beginnen deze sectie met een formele definitie van een optimalisatieprobleem:

\begin{definition}[Optimalisatieprobleem, harde beperkingen, evaluatiefunctie]
Een \emph{optimalisatieprobleem} $\OpProblem$ is een tuple $\OpProblem=\tupl{\ConfigSet=\VarDom_1\times \VarDom_2\times\ldots\times \VarDom_{\nvar},\hcfun,\evalfun}$ waarbij $\ConfigSet$ een verzameling is van een set configuraties voor $\nvar$ variabelen, $\funsig{\hcfun}{\ConfigSet}{\BBB}$ een afbeelding van zo'n configuratie naar een Booleaanse waarde, die bepaald of de configuratie voldoet aan de ``\emph{harde beperkingen}''. $\funsig{\evalfun}{\ConfigSet}{\RRR}$ stelt een \emph{evaluatiefunctie} voor die bepaald in welke mate een configuratie wenselijk is. De waarde van de evaluatie van een configuratie \fun{f}{x} wordt ook wel de \emph{fitness-waarde} genoemd.
\end{definition}
Bij een optimalisatieprobleem gaan we op zoek naar een configuratie $x\in\ConfigSet$ die aan de harde beperkingen voldoet en de evaluatiefunctie optimaliseert. Meestal maakt men het onderscheid tussen een minimalisatie en een maximalisatie. In deze thesis zullen we altijd we een bij een optimalisatieprobleem altijd streven naar een configuratie $x\in\ConfigSet$ met een zo laag mogelijk evaluatie \fun{f}{x}. We kunnen echter eenvoudig elk maximalisatieprobleem $\tupl{\ConfigSet,\hcfun,\evalfun}$ omzetten in een minimalisatieprobleem $\tupl{\ConfigSet,\hcfun,\evalfuna}$ met $\funsigimp{\evalfuna}{\ConfigSet}{\RealSet}{x}{-\fun{\evalfun}{x}}$.

\paragraph{}
Formeel zoeken we dus naar een configuratie \xstar{} die we het \emph{globaal optimum noemen}:

\begin{definition}[Globaal optimum $\bestSol$]
Een globaal optimum voor een zoekprobleem $\OpProblem=\tupl{\ConfigSet,\hcfun,\evalfun}$ is een configuratie $\bestSol$ waarbij:
\begin{equation}
\bestSol=\displaystyle\argmin_{\sol\in\ConfigValSet}\fun{\evalfun}{\sol}\mbox{ met }\ConfigValSet=\accl{\sol|\forall\sol\in\ConfigSet:\fun{\hcfun}{\sol}=\true}
\end{equation}
\end{definition}

Het is niet ongewoon dat er verschillende configuraties zijn met een gelijkaardige fitness-waarde. Dit geldt tevens voor het globaal optimum. Daarom defini\"eren we ook een optimum-set $\calXop$: een set met alle configuraties met een minimale fitness-waarde voor het probleem.

\begin{definition}[Optimum-set $\ConfigOpSet$]
Een optimum-set $\ConfigOpSet$ voor een zoekprobleem $\OpProblem=\tupl{\ConfigSet,\hcfun,\evalfun}$ is een set van geldige configuraties $\sol\in\ConfigValSet$ waarvoor geldt:
\begin{equation}
\calXop=\accl{\sol|\sol\in\ConfigValSet\wedge\fun{\evalfun}{\sol}=\fun{\evalfun}{\bestSol}}
\end{equation}
\end{definition}

\paragraph{}
In een algemeen geval kunnen de domeinen $A_i$ van de variabelen $x_i$ oneindig groot zijn en bijvoorbeeld $\RRR$ omvatten. Geen enkele machine met een eindig geheugen kan alle elementen uit een oneindige verzameling. We zullen daarom altijd de domeinen $A_i$ als eindig beschouwen. In het geval het domein van een variabele in werkelijkheid oneindig is, discretiseren we dus dit domein en beperken we het aantal elementen met een onder- en bovengrens. Indien er door discretisatie fouten worden ge\"introduceerd, kunnen we het domein fijner discretiseren.

\subsection{Complexiteit van optimalisatieproblemen}

Vermits zowel de harde beperkingen $\hcfun$ als de evaluatiefunctie $\evalfun$ hier een ``\emph{blackbox}'' zijn, zullen we om $\bestSol$ te berekenen, over een significant deel van de verzameling $\ConfigSet$ moeten itereren. We verwachten dus dat de tijdscomplexiteit om een dergelijke oplossing te vinden gelijk is aan:
\begin{equation}
\bigoh{\abs{\ConfigSet}}=\bigoh{\displaystyle\prod_{i=1}^\nvar\abs{\VarDom_i}}
\end{equation}
Indien we de assumptie maken dat alle domeinen dezelfde zijn dan bekomen we:
\begin{equation}
\bigoh{\abs{\ConfigSet}}=\bigoh{\abs{\VarDom_1}^\nvar}\mbox{ indien }\forall \VarDom_i,\VarDom_j: \VarDom_i=\VarDom_j
\end{equation}
We zien dus dat deze tijdscomplexiteit exponentieel stijgt met het aantal variabelen~$\nvar$. Optimalisatieproblemen in het algemeen liggen dan ook in \comp{NP-hard}.

\paragraph{}
Indien de vorm van de evaluatiefunctie $\evalfun$ en de harde beperkingen $\hcfun$ gedeeltelijk gekend is, kan men dit meestal uitbuiten. Daarom liggen sommige optimalisatieproblemen in \comp{P}. \algo{Karmarkars algoritme}\cite{linearProgrammingInP} bijvoorbeeld lost het \prbm{Lineaire Optimalisatie Probleem} op in \bigoh{\nvar^{3.5}L} met $\nvar$ het aantal variabelen en $L$ de diepte van de discretisatie in bits. In het geval van lineair programmeren betekent dit dat de evaluatiefunctie kan geschreven worden als het inwendig product tussen de vector van de variabelen en een vector met constanten. De harde beperkingen moeten voor te stellen zijn als een elementgewijze vergelijking tussen een constante vector en de vermenigvuldiging met de variabelen-vector en een constante matrix. Ook andere optimalisatieproblemen zoals bijvoorbeeld het \prbm{Maximum Flow Problem} en het \prbm{Minimum Spanning Tree Problem} zijn problemen die in polynomiale tijd kunnen worden opgelost.

\paragraph{}
Toch is er weinig ruimte voor optimisme. Een logische veralgemening van \prbm{Lineaire Optimalisatie} is immers \prbm{Kwadratische Optimalisatie}. Onder sommige omstandigheden kunnen we dit probleem reduceren\cite{Kozlov1980223} naar een geval van \prbm{Lineaire Optimalisatie}, maar een algemeen \prbm{Kwadratische Optimalisatie} probleem ligt in \comp{NP-hard}\cite{qpInNP}. Ook andere bekende optimalisatieproblemen zoals het \prbm{Travelling Salesman Problem (TSP)} en \prbm{Integer Programming (IP)} liggen in \comp{NP-hard}.

\paragraph{}
Tot slot dient men in de context van optimalisatieproblemen de kanttekening te maken dat het bestaan van een polynomiaal algoritme meestal niet meteen impliceert dat dit ook op kleine gevallen sneller werkt dan zijn exponenti\"ele tegenhangers. Een populaire methode bij het oplossen van \prbm{Lineaire optimalisatie} is bijvoorbeeld het \algo{Simplex}-algoritme. Klee en Minty\cite{klee:1972} stellen een geval waarbij het algoritme exponentieel veel tijd vraagt. Toch is \algo{Simplex} in de meeste gevallen sneller dan \algo{Karmarkar's algoritme}.