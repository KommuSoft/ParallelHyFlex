\chapter{Definities en State-of-the-Art}
\label{hoofdstuk:1}

In de inleiding hebben we de kort de verschillende concepten besproken die in deze thesis een belangrijke rol zullen spelen. In dit hoofdstuk gaan we hier dieper op in: we formaliseren de concepten in definities en geven een kort overzicht van de belangrijkste wetmatigheden rond deze concepten.

\section{Optimalisatieproblemen}

We beginnen deze sectie met een formele definitie van een optimalisatieprobleem:

\begin{definition}[Optimalisatieprobleem]%, harde beperkingen, evaluatiefunctie, fitness-waarde
Een optimalisatieprobleem $\OpProblem$ is een tuple $\OpProblem=\tupl{\ConfigSet=\VarDom_1\times \VarDom_2\times\ldots\times \VarDom_{\nvar},\hcfun,\evalfun}$ waarbij $\ConfigSet$ een verzameling is van een set configuraties voor $\nvar$ variabelen, $\funsig{\hcfun}{\ConfigSet}{\BBB}$ een afbeelding van zo'n configuratie naar een Booleaanse waarde, die bepaald of de configuratie voldoet aan de ``harde beperkingen''. $\funsig{\evalfun}{\ConfigSet}{\RRR}$ stelt een evaluatiefunctie voor die bepaald in welke mate een configuratie wenselijk is. De waarde van de evaluatie van een configuratie \fun{f}{x} wordt ook wel de fitness-waarde genoemd.
\end{definition}
Bij een optimalisatieprobleem gaan we op zoek naar een configuratie $x\in\ConfigSet$ die aan de harde beperkingen voldoet en de evaluatiefunctie optimaliseert. Meestal maakt men het onderscheid tussen een minimalisatie en een maximalisatie. In deze thesis zullen we altijd we een bij een optimalisatieprobleem altijd streven naar een configuratie $x\in\ConfigSet$ met een zo laag mogelijk evaluatie \fun{f}{x}. We kunnen echter eenvoudig elk maximalisatieprobleem $\tupl{\ConfigSet,\hcfun,\evalfun}$ omzetten in een minimalisatieprobleem $\tupl{\ConfigSet,\hcfun,\evalfuna}$ met $\funsigimp{\evalfuna}{\ConfigSet}{\RealSet}{x}{-\fun{\evalfun}{x}}$. Formeel zoeken we dus naar een configuratie \xstar{} die we het \emph{globaal optimum noemen}.

\begin{definition}[Globaal optimum $\bestSol$]
Een globaal optimum voor een zoekprobleem $\OpProblem=\tupl{\ConfigSet,\hcfun,\evalfun}$ is een configuratie $\bestSol$ waarbij:
\begin{equation}
\bestSol=\displaystyle\argmin_{\sol\in\ConfigValSet}\fun{\evalfun}{\sol}\mbox{ met }\ConfigValSet=\accl{\sol|\forall\sol\in\ConfigSet:\fun{\hcfun}{\sol}=\true}
\end{equation}
\end{definition}

Het is niet ongewoon dat er verschillende configuraties zijn met een gelijkaardige fitness-waarde. Dit geldt tevens voor het globaal optimum. Daarom defini\"eren we ook een optimum-set $\calXop$: een set met alle configuraties met een minimale fitness-waarde voor het probleem.

\begin{definition}[Optimum-set $\ConfigOpSet$]
Een optimum-set $\ConfigOpSet$ voor een zoekprobleem $\OpProblem=\tupl{\ConfigSet,\hcfun,\evalfun}$ is een set van geldige configuraties $\sol\in\ConfigValSet$ waarvoor geldt:
\begin{equation}
\calXop=\accl{\sol|\sol\in\ConfigValSet\wedge\fun{\evalfun}{\sol}=\fun{\evalfun}{\bestSol}}
\end{equation}
\end{definition}

\paragraph{}
In een algemeen geval kunnen de domeinen $A_i$ van de variabelen $x_i$ oneindig groot zijn en bijvoorbeeld $\RRR$ omvatten. Geen enkele machine met een eindig geheugen kan echter alle elementen uit een domein met oneindig veel elementen voorstellen. We zullen daarom altijd de domeinen $A_i$ als eindig beschouwen. In het geval het domein van een variabele in werkelijkheid oneindig is, discretiseren we dus dit domein en beperken we het aantal elementen met een onder- en bovengrens. Indien er door discretisatie fouten worden ge\"introduceerd, kunnen we deze oplossen door het domein fijner te discretiseren.
\paragraph{}
Vermits zowel de harde beperkingen $\hcfun$ als de evaluatiefunctie $\evalfun$ hier een ``\emph{blackbox}'' zijn, zullen we om $\bestSol$ te berekenen, over een significant deel van de verzameling $\ConfigSet$ moeten itereren. We verwachten dus dat de tijdscomplexiteit om een dergelijke oplossing te vinden gelijk is aan:
\begin{equation}
\bigoh{\abs{\ConfigSet}}=\bigoh{\displaystyle\prod_{i=1}^\nvar\abs{\VarDom_i}}
\end{equation}
Indien we de assumptie maken dat alle domeinen dezelfde zijn dan bekomen we:
\begin{equation}
\bigoh{\abs{\ConfigSet}}=\bigoh{\abs{\VarDom_1}^\nvar}\mbox{ indien }\forall \VarDom_i,\VarDom_j: \VarDom_i=\VarDom_j
\end{equation}
We zien dus dat deze tijdscomplexiteit exponentieel stijgt met het aantal variabelen~$\nvar$. Optimalisatieproblemen in het algemeen liggen dan ook in \comp{NP-hard}.

\subsection{Complexiteit van optimalisatieproblemen}

Sommige optimalisatieproblemen liggen in \comp{P}. \algo{Karmarkar's algoritme}\cite{linearProgrammingInP} bijvoorbeeld lost het \prbm{lineaire optimalisatie} probleem op in \bigoh{\nvar^{3.5}L} met $\nvar$ het aantal variabelen en $L$ de diepte van de discretisatie in bits. Dit komt omdat we beperkingen plaatsen op de vorm van de evaluatiefunctie $\evalfun$ en de harde beperkingen $\hcfun$. Bij lineair programmeren betekent dit dat de evaluatiefunctie kan geschreven worden als het inwendig product tussen de vector van de variabelen en een vector met constanten. Het harde beperkingen moeten voor te stellen zijn zodat wanneer we de vector met de variabelen vermenigvuldigen met een matrix met constante elementen, alle elementen in de resulterende vector kleiner zijn dan een andere vector met constante elementen. Ook andere optimalisatieproblemen zoals bijvoorbeeld \prbm{Maximum Flow} en \prbm{Minimum Spanning Tree} zijn problemen die met polynomiale algoritmen kunnen worden opgelost.

\paragraph{}
Toch is er weinig ruimte voor optimisme. Een logische veralgemening van \prbm{Lineaire optimalisatie} is immers \prbm{Kwadratische optimalisatie}. Onder sommige omstandigheden kunnen we dit probleem reduceren naar een geval van \prbm{Lineaire Optimalisatie}\cite{Kozlov1980223}, maar een algemeen \prbm{Kwadratisch optimalisatie} probleem ligt in \comp{NP-hard}\cite{qpInNP}. Ook andere bekende optimalisatieproblemen zoals \prbm{Travelling Salesman Problem (TSP)} en \prbm{Integer Programming (IP)} liggen in \comp{NP-hard}.

\paragraph{}
Tot slot dient men in de context van optimalisatieproblemen een kanttekening maken dat een polynomiaal algoritme meestal niet meteen impliceert dat dit ook op kleine gevallen sneller werkt dan zijn exponenti\"ele tegenhangers. Een populaire methode bij het oplossen van \prbm{Lineaire optimalisatie} is bijvoorbeeld het \algo{Simplex}-algoritme. Klee en Minty\cite{klee:1972} construeerden echter een een geval waarbij het algoritme exponentieel veel tijd vraagt. Toch is \algo{Simplex} in de meeste gevallen sneller dan \algo{Karmarkar's algoritme}.

\section{Heuristieken}

Hoewel de meeste optimalisatieproblemen \comp{NP-hard} zijn, is in een praktische context de configuratie met een optimale evaluatiefunctie net van cruciaal belang. Voor de meeste toepassingen is een configuratie die aan de harde beperkingen voldoet en de fitness-waarde van de echte oplossing benadert voldoende. In dat geval wordt meestal een heuristiek ge\"implementeerd:

\begin{definition}[Heuristiek]
Een heuristiek is een programma die gegeven een optimalisatieprobleem $\OpProblem=\tupl{\ConfigSet,\hcfun,\evalfun}$ een oplossing berekent $\goodSol$ in een redelijke tijd. Doorgaans voldoet deze oplossing aan de harde beperkingen ($\fun{\hcfun}{\goodSol}=\true$) en ligt de voorgestelde oplossing $\goodSol$ in fitness-waarde niet ver van de werkelijke oplossing $\bestSol$.
\end{definition}

Deze definitie blijft redelijk vaag en geeft dan ook veel ruimte voor interpretatie. Doorgaans verwachten we dat het algoritme stop in polynomiale tijd en in de meeste gevallen worden er ook beperkingen gezet op hoe ver de fitness-waarde $\fun{\evalfun}{\goodSol}$ mag afwijken van de optimale fitness-waarde $\fun{\evalfun}{\bestSol}$, al zijn beide voorwaarden niet strikt noodzakelijk. Minsky\cite{minskyHeuristic} schrijf hierover:
\begin{quote}
``Hints'', ``suggestions'', or ``rules of thumb'', which only usually work are called heuristics. A program which works on such a basis is called a heuristic program. It is difficult to give a more precise definition of heuristic program - this is to be expected in the light of Turing's demonstration that there is no systematic procedure which can distinguish between algorithms (programs that always work) and programs that do not always work.
\end{quote}

% Een belangrijke theorema stelt dat fout onmogelijk constant kan zijn:
% \begin{theorem}
% Voor geen enkele heuristiek bestaat geen getal $E$ waarvoor geldt:
% \begin{quote}
% dat voor elke probleem-instantie van het probleem, $\fun{f}{\xdot}-\fun{f}{L}\leq L$.
% \end{quote}
% \end{theorem}


\section{Metaheuristieken}

\subsection{Problemen met heuristieken}

Heuristieken bieden een antwoord door een algoritme uit te voeren die in polynomiale tijd een oplossing zal uitrekenen. In het geval het algoritme snel genoeg is, en we kunnen leven met de garanties die dit algoritme biedt, is dit een acceptabele methode. In de meeste gevallen is dit echter niet zo. Praktische problemen zijn groot en complex en doorgaans kan een heuristiek weinig garanties bieden. Een ander probleem met heuristieken is dat ze weinig aanpasbaar zijn: stel dat we een bepaalde tijd ter beschikking stellen, dan zal het algoritme zich doorgaans niet aan deze beperking houden: ofwel loopt het algoritme eerder af en maakt het dus geen gebruik van alle beschikbaargestelde rekentijd, indien het algoritme niet afloopt binnen de gespecificeerde tijd is er ook geen sprake van een parti\"ele oplossing.

\subsection{Formele definitie}

Metaheuristieken proberen deze problemen op te lossen. We geven eerst een formele definitie van een metaheuristiek waarna we relevante terminologie invoeren.

\begin{definition}[Metaheuristiek]
\deflab{metaheuristic}
Een metaheuristiek is een algoritme die een oplossingsruimte $\SolSet\subseteq\accl{\sol|\forall \sol\in \ConfigSet:\fun{\hcfun}{\sol}}\subseteq\ConfigSet$ beschouwd met $\bestSol\in\SolSet$. Verder beschouwd het \'e\'en of meer overgangsfuncties $h_i:\SolSet^{k_i}\times\RealSet^{l_i}\rightarrow\SolSet$. De metaheuristiek werkt door \'e\'en of meerdere instanties $s_1,s_2,\ldots,s_j$ uit $\SolSet$ te genereren. En vervolgens herhaaldelijk deze overgangsfuncties toe te passen op deze instanties. De resultaten van deze functietoepassingen kun gebruikt worden in andere toepassingen van de overgangsfuncties. Het algoritme stopt wanneer aan een bepaalde stopconditie voldaan is (bijvoorbeeld: het algoritme draait een bepaalde tijd op de machine). Hierna wordt de oplossing met de beste fitness-waarde als uitvoer teruggegeven.
\end{definition}

Op basis van deze definitie kunnen we ook een algoritme op hoog niveau opstellen zoals beschreven in \algref{metaheuristicGeneral}.

\begin{algorithm}[H]
 \SetAlgoLined
 %\KwData{}
 %\KwResult{how to write algorithm with \LaTeX2e }
 Bereken een initi\"ele set van oplossingen $\PopSet_1$ en hun fitness-waarde\;
 $b_1\leftarrow\displaystyle\argmin_{\sol\in\PopSet_1}{\fun{\evalfun}{\sol}}$\;
 \Repeat{het stopcriterium is bereikt}{
  Genereer op basis van $\PopSet_t$ en $t$ stochastisch een nieuwe set oplossingen $\PopSet_{t+1}$ samen met hun fitness-waarde\;
  $b_{t+1}\leftarrow\displaystyle\argmin_{\sol\in\PopSet_{t+1}\cup\accl{b_t}}{\fun{\evalfun}{\sol}}$\;
  $t\leftarrow t+1$\;
 }
 \KwRet{$b_t$}
 \caption{Hoog niveau beschrijving van een metaheuristiek\cite{DBLP:journals/jc/ShonkwilerV94}.}
 \alglab{metaheuristicGeneral}
\end{algorithm}

Typisch aan metaheuristieken is een (sterke) aanwezigheid van toevalsgetallen. Zo hebben we bij de signatuur van de overgangsfuncties ook een set re\"ele getallen opgenomen. Deze getallen worden door het toeval gegenereerd en bepalen mee het resultaat van de functie. Een belangrijk concept die we hiermee kunnen introduceren is de omgeving ofwel ``\emph{neighborhood}'' van een overgangsfunctie:

\begin{definition}[Omgeving van een overgangsfunctie]
De omgeving van een overgangsfunctie is de verzameling van alle oplossingen die we kunnen genereren vanuit een gegeven set van oplossingen ongeacht de waarde van de toevalsfactoren. De omgeving van een functie $h_i$ is dus:
\begin{equation}
\fun{\neighbr_{h_i}}{s_1,s_2,\ldots,s_{k_i}}=\accl{s|s=\fun{h_i}{s_1,s_2,\ldots,s_{k_i},\xi_1,\xi_2,\ldots,\xi_{l_i}}}
\end{equation}
\end{definition}

\subsubsection{Local Search}
Het concept van een omgeving is belangrijke omdat het meteen ook een populaire zoekstrategie voor metaheuristieken introduceert: \emph{lokaal zoeken} ofwel ``\emph{local search (LS)}''. Deze zoekstrategie vertrekt van een gegeven oplossing en zoekt - volgens een bepaalde omgeving - alle oplossingen in de buurt af op zoek naar een betere oplossing. In het geval we zo'n oplossing vinden wordt deze oplossing de nieuwe oplossing en beginnen we vervolgens een zoektocht rond de nieuwe oplossing. Local Search komt voor in twee smaken: ``\emph{first-improvement}'' en ``\emph{best-improvement}''. In het geval van \emph{first-improvement} wordt de eerste oplossing die beter is dan de huidige oplossing de nieuwe actieve oplossing. In het geval van \emph{best-improvement} doorzoeken we de volledige omgeving en migreren we naar de beste oplossing in de omgeving.
\paragraph{}
Local Search is een krachtige optimalisatietechniek die doorgaans tot acceptabele resultaten kan leiden. Het probleem zit hem in het idempotente karakter van local search: \'e\'enmaal we local search op een oplossing hebben toegepast heeft een tweede maal local search toepassen met dezelfde omgevings-definitie geen zin: we weten immers dat er in de omgeving geen betere oplossingen gevonden zullen worden want anders was het algoritme de vorige keer niet bij deze oplossing gestopt. Omwille van deze reden kan local search zelf geen volwaardige metaheuristiek worden genoemd. We verwachten immers dat als we meer tijd investeren, we op termijn altijd tot betere resultaten $\xdot$ ofwel de echte oplossing $\xstar$ zullen komen. Local search vormt echter de basis voor een groot aantal metaheuristieken die doorgaans een mechanisme implementeren om de oplossing uit het lokaal optimum te laten migreren.

\subsection{De ``Zoo van de Metaheuristieken''}

\defref{metaheuristic} is vrij abstract. Daarom zullen we in deze sectie de belangrijkste families van metaheuristieken kort beschouwen.

\subsubsection{Genetische Algoritmes}

\emph{Genetische algoritmen} ofwel ``\emph{Genetic Algorithms (GA)}'' zijn de eerste familie van metaheuristieken en werken op basis van een populatie: een set van oplossingen. Uit deze populatie worden twee of meer oplossingen geselecteerd. De selectie is meestal gerelateerd aan de fitness-waarde van deze oplossingen. Vervolgens past men een recombinatie-operator toe: een overgangsfunctie die twee of meer oplossingen als invoer neemt en op basis hiervan een nieuwe oplossing genereert die eigenschappen van alle ouders deelt. Doorgaans past men op deze oplossing ook een mutatie toe: men zal de oplossing lichtjes aanpassen door middel van een andere overgangsfunctie. Vervolgens de oplossing onder bepaalde voorwaarden in de populatie ge\"injecteerd. In ruil voor de nieuwe oplossing zal de populatie ook \'e\'en of meer oplossingen uit de populatie verwijderen.

\subsubsection{Simulated Annealing}

\emph{Simulated Annealing (SA)} is de eerste gepubliceerde metaheuristiek. Het is een schema gebaseerd op het \algo{Algoritme van Metropolis}. Het werkt aan de hand van \'e\'en overgangsfunctie en \'e\'en oplossing die soms de actieve oplossing wordt genoemd. Het algoritme voert telkens de overgangsfunctie uit op de actieve oplossing. De resulterende oplossing beschouwen we als de nieuwe oplossing met een kans:
\begin{equation}
\fun{p}{\mbox{accept $s_1\rightarrow s_2$}}=\fun{\min}{1,e^{\brak{\fun{\evalfun}{s_2}-\fun{\evalfun}{s_1}}/T}}
\end{equation}

Deze formule stelt dat indien de nieuwe oplossing beter is dan de oude oplossing, we altijd accepteren. Indien de nieuwe oplossing minder gunstig is accepteren we met een bepaalde kans die kleiner wordt naarmate het verschil in fitness-waarde groeit. In de formule staat ook een onbekende parameter $T$ ofwel \emph{temperatuur}. De temperatuur bepaalt hoe sterk de kans daalt naarmate de kloof groeit. Het is een variabele die initieel op een positief getal wordt gezet en gedurende het zoekproces langzaam naar 0 zakt. Dit betekent dat we in het begin sterk geneigd zijn om een slechtere oplossing te accepteren. Op het einde accepteren we bijna uitsluitend oplossing die beter zijn dan hun ouder. Hoe de temperatuur concreet evolueert kan vrij ingesteld worden en introduceert dan ook een groot aantal varianten.

\subsubsection{Iterated Local Search}

In de vorige sectie bespraken we \emph{Local Search}. Het probleem met \emph{Local Search} is echter dat het convergeert naar een lokaal minimum en vanaf dat moment geen vooruitgang meer kan maken. \emph{Iterated Local Search (ILS)} is een zoekproces dat afwisselt tussen twee fases: in de \emph{local search}-fase optimaliseert het programma de oplossing tot het in een lokaal optimum terecht komt; in de \emph{perturbatie}-fase voert men een overgangsfunctie uit die met een zekere kans in staat moet zijn om een oplossing te genereren die uit het lokale optimum ontsnapt.

\subsubsection{Variable Neighbourhood Search}

\emph{Variable Neighbourhood Search (VNS)} is een metaheuristiek die verder bouwt op \emph{Iterated Local Search}. In het geval van \emph{Variable Neighbourhood Search} is er echter sprake van verschillende definities voor een omgeving $\neighbr_i$ in een iteratie zullen in de \emph{shake}-fase migreren naar een random oplossing in de omgeving $\neighbr_i$ en vervolgens passen we \emph{local search} toe op basis van deze omgeving. In het geval we tot een betere oplossing komen, wordt de eerste omgeving $\neighbr_1$ opnieuw de actieve omgeving. In het andere geval kiezen we de volgende omgeving $\neighbr_{i+1}$. Op het moment dat alle omgevingen zijn uitgeput beginnen we ook opnieuw bij $\neighbr_1$.%in het geval we een lokaal optimum volgens omgeving $\calN_i$ bereiken, zullen we \emph{local search} toepassen op basis van de volgende omgeving $\calN_i$ in de hoop dat deze omgeving ons naar een betere oplossing zal leiden. In het geval alle

\subsubsection{Tabu Search}

\emph{Tabu Search}

\subsection{Classificatie: een kaart voor de ``Zoo van de Metaheuristieken''}

\subsection{Modelleren van Metaheuristieken}

Elke metaheuristiek die we hierboven beschreven hebben dient telkens drie belangrijke vragen in het achterhoofd te houden\cite{DBLP:journals/jc/ShonkwilerV94}:
\begin{enumerate}
 \item kan het globale optimum altijd gevonden worden door de metaheuristiek,
 \item hoe kunnen we identificeren dat we het globale optimum gevonden hebben, en
 \item hoe lang zal het duren alvorens we dit optimum gevonden hebben
\end{enumerate}

Wanneer we geen details kennen in verband met de vorm van de evaluatiefunctie $\evalfun$ spreekt het voor zich dat we enkel zeker kunnen zijn dat we een globaal optimum hebben gevonden door middel van \emph{exhaustive search}. Ook wat betreft de derde vraag verwachten we een \emph{exhaustive search} proces alvorens we zeker zijn dat we het optimum hebben gevonden. De verwachtte tijd alvorens dit gebeurt kan echter sterk verschillen van de tijd die we dienen te besteden aan het doorzoeken van een significant deel van de zoekruimte. Berekenen wanneer we gemiddeld dit optimum zullen bereiken staat bekend als het \prbm{Hitting Time Problem}. Om dit probleem te formaliseren dienen we eerst de notie van de raaktijd van een Markov-keten te defini\"eren:

\begin{definition}[Raaktijd $\hittime$]
We defini\"eren de raaktijd $\hittime$ van een Markov-keten $\PopChain$ die bestaat uit populaties $\PopSet_t$ als:
\begin{equation}
\fun{\hittime}{\PopChain}=\min\accl{t|\PopSet_t\in\PopChain\wedge\ConfigOpSet\cap \PopSet_t\neq\emptyset}
\end{equation}
We kunnen deze definitie ook uitbreiden naar de raaktijd voor een zekere fitness-waarde $v$:
\begin{equation}
\fun{\hittime}{\PopChain,v}=\min\accl{t|\PopSet_t\in\PopChain:\exists \sol\in \PopSet_t:\fun{\evalfun}{\sol}\leq v}
\end{equation}
\end{definition}

Het \prbm{Hitting Time Problem} gaat vervolgens op zoek naar de raaktijd van een gemiddelde Markov-keten.

\subsubsection{Verwachtte tijd van een sequentieel proces}

Op basis van \algref{metaheuristicGeneral} kunnen we uitspraken doen, zowel over de verwachtte raaktijd wanneer we het zoeken uitbesteden aan \'e\'en processor of uitbesteden aan verschillende processoren die elk onafhankelijk een eigen zoekproces laten lopen. Onderzoek naar de raaktijd is gepubliceerd in \cite{DBLP:journals/jc/ShonkwilerV94}. Hiervoor zal men een Markov-model beschouwen. De knopen van dit Markov-model stellen populaties voor: een verzameling van oplossingen. We kunnen hier dus denken aan een verzameling $\PopSet_i$ in \algref{metaheuristicGeneral}. Als beperking stellen we dat de populatie een vaste grootte heeft. Deze beperking is redelijk vermits elke praktische machine een eindig geheugen heeft en dus ook maar een eindig aantal oplossingen tegelijk kan beschouwen. Verder bevat ons Markov-model ook overgangskansen $\fun{p}{t,i,j}$: de kans om van een populatie $\PopSet_i$ naar een populatie $\PopSet_j$ te gaan op tijdstip $t$. Indien de waarde van $\fun{p}{i,j,t}$ constant blijft over de tijd $t$ dan spreken we over een \emph{stationair} proces (dit is typisch voor bijvoorbeeld een genetisch algoritme), indien de kansen in de tijd veranderen beschouwen we een \emph{niet-stationair} proces.

\paragraph{}

Indien een populatie een oplossing bevat met een fitness-waarde die kleiner of gelijk is aan de waarde die we zoeken $v$, spreken we over een \emph{doel-populatie}, in het andere geval zullen we spreken over een \emph{normale populatie}. We kunnen zonder verlies van algemeenheid de populaties zo ordenen dat de populaties $\PopSet_1,\PopSet_2,\ldots,\PopSet_g$ doel-populaties zijn, en de populaties $\PopSet_{g+1},\PopSet_{g+2},\ldots,\PopSet_N$ populaties waar er geen gewenste oplossing in zit. In dat geval kunnen we de transitiematrix $P$ opdelen in submatrices:
\begin{equation}
P=\brak{\begin{array}{cc}
J&H\\
B&\hat{P}
\end{array}}
\end{equation}
Hierbij stelt $J$ een $g\times g$ voor, $H$ een $g\times N-g$, $B$ een $N-g\times g$ matrix en $\hat{P}$ logischerwijs een $N-g\times N-g$ matrix. Het spreekt voor zich dat de waarde van $J$ en $H$ niet zo interessant zijn: eenmaal we ons in een doel-populatie bevinden kunnen we immers stoppen met het algoritme uit te voeren. Daarom zullen we stellen dat $J=\identitymatrix[g\times g]$ en $H=\zeromatrix[g\times N-g]$. We streven er verder naar om met ons algoritme de waardes in $B$ -- ook wel de \emph{bridge} genoemd -- zo hoog mogelijk te maken: deze matrix bevat immers de kansen om van naar een doel-populatie te migreren. $\hat{P}$ wordt ook wel de \emph{verwijderde transitiematrix} genoemd. De waardes in de matrix worden be\"invloed door het probleem (via de definities van de omgevingen) en het zoekalgoritme (de metaheuristiek in kwestie).
\paragraph{}
De kwaliteit van een zoekmethode na $t$ stappen kunnen we uitdrukken met een probabiliteitsvector $\vec{\alpha}_t$. $\alpha_{t,i}$ geeft aan met hoeveel kans het algoritme in tijdstap $t$ de populatie $\PopSet_i$ beschouwt. Net als bij de matrix delen we de probabiliteitsvector op in twee delen: een gedeelte $\vec{\alpha}^G_t$ met dimensie $g$ en een gedeelte $\vec{\hat{\alpha}}_t$. Op basis van de transitiematrix kunnen we de probabiliteitsvector na $t$ stappen berekenen in functie van $\alpha_0$:
\begin{equation}
\vec{\alpha}_t=\brak{\vec{\alpha}_t^G\ |\ \vec{\hat{\alpha}}_t}=\vec{\alpha}_{t-1}\cdot P=\brak{\vec{\alpha}_{t-1}^G+B\cdot\vec{\hat{\alpha}}_{t-1}\ |\ \vec{\hat{\alpha}}_{t-1}\cdot\hat{P}}
\eqnlab{oneStepTransition}
\end{equation}
We zijn vooral ge\"interesseerd in de fractie van Markov-ketens die in stap $t$ naar een doel-populatie migreren ofwel de kans dat we stap $t$ een gewenste oplossing bereiken:
\begin{equation}
\Prob{\theta=t}=\displaystyle\sum_{i=g+1}^{N}\brak{\alpha_{t-1,i}\cdot\displaystyle\sum_{j=1}^{g}\fun{p}{t,i,j}}=\vec{\hat{\alpha}}_{t-1}\cdot B\cdot\onematrix[1\times N-g]
\eqnlab{exhaust}
\end{equation}
Een ander logische gevolg uit \eqnref{oneStepTransition} zijn de transities van normale populaties naar andere normale populaties. De som van de kansen die in deze deelvector aanwezig zijn, is de kans dat we na deze $t$ stappen nog steeds geen acceptabele oplossing gevonden hebben:
\begin{eqnarray}
\vec{\hat{\alpha}}_t=\vec{\hat{\alpha}}_{t-1}\cdot\hat{P}=\vec{\hat{\alpha}}_{t-2}\cdot\hat{P}^2=\ldots=\vec{\hat{\alpha}}_{0}\cdot\hat{P}^t\eqnlab{remainder}\\
\Prob{\theta>t}=\displaystyle\sum_{i=g+1}^{N}\alpha_{t,i}=\vec{\hat{\alpha}}_t\cdot\onematrix[1\times N-g]=\vec{\hat{\alpha}}_{0}\cdot\hat{P}^t\cdot\onematrix[1\times N-g]\eqnlab{nothit}
\end{eqnarray}
Op basis van \eqnref{remainder} en \eqnref{nothit} kunnen we nu de verwachte stap berekenen waarop we een gewenste oplossing zullen berekenen:
\begin{equation}
\mean{\theta}=1+\displaystyle\sum_{i=0}^{\infty}{\Prob{\theta>i}}=1+\displaystyle\sum_{i=0}^{\infty}{\vec{\hat{\alpha}}_{0}\cdot\hat{P}^i\cdot\onematrix[1\times N-g]}
\eqnlab{meanHit}
\end{equation}
In deze vergelijking tellen we elke kans $\Prob{\theta=k}$ juist $k$ keer.
\paragraph{}
Vermits $\hat{P}$ een matrix met kansen voorstelt is elke element $\hat{p}_{i,j}\geq 0$. We maken verder de assumptie dat we na een arbitrair aantal stappen $m$, vanuit elke normale populatie $\calP_i$ een kans bestaat dat we naar een andere normale populatie $\calP_j$ kunnen migreren. Vermits hierdoor de matrix primitief is, en er minstens \'e\'en rij van $\hat{P}$ niet sommeert naar 1 (anders zouden we nooit in een doel-populatie kunnen terechtkomen), weten we dat de grootste eigenwaarde $\ev[\hat{P}]<1$.

% Een theorema stelt:
% \begin{theorem}
% stel de twee grootste eigenwaardes $\lambda_1>\abs{\lambda_2}$ van een $n\times n$ matrix $A$. Dan bestaat er een vaste veelterm $h$ met een graad $0\leq\funm{deg}{h}< n-1$ zodat:
% \begin{equation}
% \forall k=1,2,\ldots:\abs{\displaystyle\frac{\vec{x}\cdot A\cdot\onematrix[1\times n]}{\lambda_1^k}-\vec{x}\cdot\vec{v}}<\fun{h}{k}\cdot\abs{\displaystyle\frac{\lambda_2}{\lambda_1}}^k
% \end{equation}
% Met $\vec{x}$ een willekeurige vector en $\vec{v}$ de rechtse eigenvector van $\lambda_1$.
% \thelab{eigenValuesMatrix}
% \end{theorem}
\paragraph{}
We kunnen een eigenvector benaderen door een $\onematrix$-vector een groot aantal maal toe te passen op de matrix in kwestie en vervolgens te normaliseren. Daarom kunnen we stellen dat voor een grote $t$:
\begin{equation}
\begin{array}{cc}
\vec{x}\cdot\hat{P}^t\cdot\onematrix[1\times N-g]\approx\vec{x}\cdot\ev[1,\hat{P}]^t\cdot\evr[\hat{P}]=\ev[1,\hat{P}]^t\cdot\sigma&\mbox{($t\gg 1$)}
\end{array}
\eqnlab{longTermAlpha}
\end{equation}
Wanneer we als $\vec{x}$ de begindistributie $\vec{\hat{\alpha}}_0$ nemen, kunnen we het dot-product tussen $\vec{\hat{\alpha}}_0$ en $\evr[\hat{P}]$ defini\"eren als $\sigma$. We kunnen vervolgens \eqnref{meanHit} en \eqnref{longTermAlpha} samennemen in een nieuwe vergelijking die
%ons met behulp van \theref{eigenValuesMatrix}
een benaderende waarde voor de gemiddelde raaktijd oplevert:
\begin{equation}
\mean{\theta}\approx 1+\vec{\hat{\alpha}}_0\cdot\onematrix+\sigma\cdot\brak{\displaystyle\sum_{i=1}^{\infty}\lambda_{1,\hat{P}}^i}=1+\vec{\hat{\alpha}}_0\cdot\onematrix+\displaystyle\frac{\sigma\cdot \lambda_{1,\hat{P}}}{1-\lambda_{1,\hat{P}}}
\end{equation}
Net zoals we de termen van dit gemiddelde benadert hebben aan de hand van \eqnref{longTermAlpha}, kunnen we de eerste termen benaderen door deze vergelijking omgekeerd toe te passen. We stellen dus dat $1+\vec{\hat{\alpha}}_0\cdot\onematrix=\sigma\cdot\brak{1+\lambda^{-1}}$. Dit levert ons een ruwe schatting voor de raaktijd op:
\begin{equation}
\mean{\hittime}\approx\displaystyle\frac{\sigma}{\brak{1-\lambda_{1,\hat{P}}}\cdot\lambda_{1,\hat{P}}}
\eqnlab{hitTimeSingle}
\end{equation}

\subsubsection{Onafhankelijke parallelle Monte-Carlo simulaties}

\cite{DBLP:journals/jc/ShonkwilerV94} toont ook een logische stap tot parallellisatie: $p$ processoren draaien elk onafhankelijk van elkaar een Monte-Carlo simulatie. We kunnen dit fenomeen modelleren met behulp van een Markov-keten waarbij elke knoop van de keten een $p$-tuple voorstelt met de populaties van de verschillende processoren in de tijdstap. Vervolgens defini\"eren we \emph{parallelle raaktijd} $\phittime$ als volgt:

\begin{definition}[Parallelle raaktijd $\phittime$]
We defini\"eren de parallelle raaktijd $\phittime$ van een Markov-keten $\PopChain$ die bestaat uit $p$-tuples van populaties $\tupl{\PopSet_{t,1},\PopSet_{t,2},\ldots,\PopSet_{t,p}}$ als:
\begin{equation}
\fun{\phittime}{\PopChain}=\min\accl{t|\tupl{\PopSet_{t,1},\PopSet_{t,2},\ldots,\PopSet_{t,p}}\in\PopChain\wedge\exists i:\ConfigOpSet\cap \PopSet_{t,i}\neq\emptyset}
\end{equation}
We kunnen deze definitie ook uitbreiden naar de raaktijd voor een zekere fitness-waarde $v$:
\begin{equation}
\fun{\phittime}{\PopChain,v}=\min\accl{t|\tupl{\PopSet_{t,1},\PopSet_{t,2},\ldots,\PopSet_{t,p}}\in\PopChain\wedge\exists i\exists \sol\in\PopSet_{t,i}:\fun{\evalfun}{\sol}\leq v}
\end{equation}
\end{definition}
Vermits de verschillende processoren een onafhankelijk en equivalent proces uitvoeren, kunnen we gebruik maken van het theorema van de onafhankelijke kansen met de onafhankelijke EN-regel:
\begin{equation}
\begin{aligned}
\Prob{\phittime\geq t}&=\Prob{\hittime_1\geq t\wedge\hittime_2\geq t\wedge\ldots\wedge\hittime_p\geq t}\\
&=\Prob{\hittime_1\geq t}\cdot\Prob{\hittime_2\geq t}\cdot\ldots\cdot\Prob{\hittime_p\geq t}\\
&=\Prob{\hittime\geq t}\cdot\Prob{\hittime\geq t}\cdot\ldots\cdot\Prob{\hittime\geq t}=\Prob{\hittime\geq t}^p
\end{aligned}
\eqnlab{probabilityPower}
\end{equation}
Op basis van deze regel kunnen we de gemiddelde parallelle raaktijd bepalen op basis van \eqnref{meanHit}:
\begin{equation}
\begin{aligned}
\mean{\phittime}&=1+\displaystyle\sum_{i=0}^{\infty}{\Prob{\phittime>i}}=1+\displaystyle\sum_{i=0}^{\infty}{\Prob{\hittime>i}^p}\\
%&=1+\displaystyle\sum_{i=0}^{\infty}{\vec{\hat{\alpha}}_{0}\cdot\hat{P}^i\cdot\onematrix[1\times N-g]}\\
&\approx 1+\brak{\vec{\hat{\alpha}}_0\cdot\onematrix}^p+\sigma^p\cdot\brak{\displaystyle\sum_{i=1}^{\infty}\lambda_{1,\hat{P}}^{p\cdot i}}=1+\brak{\vec{\hat{\alpha}}_0\cdot\onematrix}^p+\displaystyle\frac{\sigma^p\cdot\lambda_{1,\hat{P}}^p}{1-\lambda_{1,\hat{P}}^p}\\
&\approx\displaystyle\frac{\sigma^p}{\brak{1-\lambda_{1,\hat{P}}^p}\cdot\lambda_{1,\hat{P}}^{p}}
%\Prob{\phittime\geq t}&=\Prob{\hittime_1\geq t\wedge\hittime_2\geq t\wedge\ldots\wedge\hittime_p\geq t}\\
%&=\Prob{\hittime_1\geq t}\cdot\Prob{\hittime_2\geq t}\cdot\ldots\cdot\Prob{\hittime_p\geq t}\\
%&=\Prob{\hittime\geq t}\cdot\Prob{\hittime\geq t}\cdot\ldots\cdot\Prob{\hittime\geq t}=\Prob{\hittime\geq t}^p
\end{aligned}
\eqnlab{probabilityPower}
\end{equation}
De \emph{speed-up} is bijgevolg gelijk aan:
\begin{equation}
\mbox{speed-up}=\displaystyle\frac{\mean{\hittime}}{\mean{\phittime}}=\displaystyle\frac{\lambda_{1,\hat{P}}^{p-1}}{\sigma^{p-1}}\cdot\displaystyle\frac{1-\lambda_{1,\hat{P}}^{p}}{1-\lambda_{1,\hat{P}}}\approx\displaystyle\frac{p\cdot\lambda^{p-1}}{\sigma^{p-1}}
\eqnlab{speedupMetaheuristic}
\end{equation}
In het laatste deel van de vergelijking vervangen we ook $\ev[\hat{P}]$ door $\lambda$. Dit is louter om de formules die hiervan afgeleid zijn leesbaar te houden.

\subsubsection{Semantiek van parameters $\sigma$ en $\lambda$}

\eqnref{speedupMetaheuristic} formaliseert de verwachte \emph{speed-up} bij een probleem. Deze formule bevat 3 parameters en kan daarom niet zomaar ge\"interpreteerd worden. Behalve het aantal parameters $p$ is het niet triviaal om de overige parameters te interpreteren.

\paragraph{}
We hebben in \eqnref{longTermAlpha} een benadering geformuleerd voor de distributie over normale populaties. Elke initi\"ele distributie zal bijgevolg altijd convergeren naar een distributie proportioneel aan de sterkste linkse eigenvector $\evl[\hat{P}]$. Eenmaal $\vec{\hat{\alpha}}_t\approx\evl[\hat{P}]$ kunnen we stellen dat:
\begin{equation}
\conditional{\vec{\hat{\alpha}}_{t+1}=\vec{\hat{\alpha}}_t\cdot\hat{P}=\lambda\cdot\vec{\hat{\alpha}}_{t}}{$t\gg 1$}
\end{equation}
Bijgevolg kunnen we $\lambda$ interpreteren als de kans op lange termijn dat we vanuit een normale populatie de volgende stap niet naar een doel-populatie migreren. Voor elk zinvol probleem kunnen we stellen dat $\lambda$ dicht bij $N-g/N\approx 1$ zal blijven (dit is ook de oorzaak van de laatste vereenvoudiging in \eqnref{speedupMetaheuristic}). Verder geldt altijd: $0\leq\lambda<1$.

\paragraph{}

We kunnen ook de rechtse dominante eigenvector $\evr[\hat{P}]$ beschouwen. Vermits eigenvectoren geen karakteristieke lengte hebben, beschouwen we $\dabs{\evr[\hat{P}]}_1=1$. Vermits het volgende geldt:
\begin{equation}
\conditional{A^t\approx\ev[A]^t\cdot\evr[A]\cdot\evl[A]^T}{$t\gg1$}
\end{equation}
Kunnen we stellen dat de genormaliseerde rechtse eigenvector een probabiliteitsvector is die aangeeft in welke mate populaties $i$ bijdragen tot het behoudt in een normale populatie. Wanneer we dus de $i$-de gewone populatie beschouwen en het $i$-de element van $\evr[\hat{P}]$ is klein, is er een grote kans dat we in een doel-toestand terecht zullen komen.

\paragraph{}

De interpretatie van de rechtse eigenvector vormt de basis voor de interpretatie van $\sigma$. Vermits $\sigma$ het dot-product is tussen de probabiliteitsvector $\vec{\hat{\alpha}}_0$ en $\evr[\hat{P}]$. Dit dot-product is proportioneel met de cosinus van de hoek tussen beide vectoren:
\begin{equation}
\fun{\cos}{\vec{\hat{\alpha}}_0,\evr[\hat{P}]}=\displaystyle\frac{\vec{\hat{\alpha}}_0^T\cdot\evr[\hat{P}]}{\dabs{\vec{\hat{\alpha}}_0}\cdot\dabs{\evr[\hat{P}]}}=\displaystyle\frac{\sigma}{\dabs{\vec{\hat{\alpha}}_0}\cdot\dabs{\evr[\hat{P}]}}
\end{equation}
Uit de definitie van de cosinus kunnen we bovendien stellen dat:
\begin{equation}
\dabs{\evr[\hat{P}]}=\displaystyle\frac{\dabs{\evl[\hat{P}]}}{\fun{\cos}{\evl[\hat{P}],\evr[\hat{P}]}}
\end{equation}
Bijgevolg kunnen we stellen dat:
\begin{equation}
\sigma=\displaystyle\frac{\dabs{\hat{\alpha}_0}\cdot\fun{\cos}{\vec{\hat{\alpha}}_0,\evr[\hat{P}]}}{\dabs{\evl[\hat{P}]}\cdot\fun{\cos}{\evl[\hat{P}],\evr[\hat{P}]}}
\eqnlab{sigmaMeaning}
\end{equation}
We kunnen dus stellen dat $\sigma$ de ratio is tussen de projecties van enerzijds de initi\"ele kansverdeling $\vec{\hat{\alpha}}$ en anderzijds de linkse eigenvector $\evl[\hat{P}]$ op de rechtse eigenvectoren $\evr[\hat{P}]$. Wanneer $\sigma$ klein is verwachten we sneller een oplossing te vinden. De factoren die hiertoe bijdragen zijn dus:
\begin{itemize}
 \item Een kleine $\dabs{\vec{\hat{\alpha}}_0}$. Vermits de som van $\vec{\hat{\alpha}}_0$ neerkomt om de kans dat onze beginpopulatie geen doelpopulatie is, streven we er naar om deze zo laag mogelijk te houden.
 \item Wanneer $\vec{\hat{\alpha}}_0$ en $\evr[\hat{P}]$ niet gelijkaardig zijn, dan zijn de populaties waar we met een grote kans in starten niet de populaties die met een grote kans naar een normale populatie migreren in de volgende stap. Wanneer de cosinus tussen de vectoren dus klein is, verwachten we in de eerste stappen naar een doel-populatie te migreren.
 \item Wanneer de linkse en de rechtse eigenvector gelijk zijn, is de matrix $\hat{P}$ symmetrisch. Indien de matrix symmetrisch is, verwachtten we doorgaans minder populaties waarnaar we kunnen migreren, maar vervolgens niet uit kunnen geraken. Indien de matrix immers symmetrisch is, is de kans om vanuit een populatie in een andere populatie te migreren immers dezelfde om er vervolgens terug uit te geraken. Daarom streven we naar een zo minimaal mogelijk hoek tussen de twee.
\end{itemize}
Uit de formule kunnen we afleiden dan $\sigma$ aan slechts \'e\'en kant begrensd is: $0<\sigma$.

\subsubsection{Superlineaire Speed-up}

Vermits $0\leq\lambda<1$ en $0<\sigma$, kunnen we met behulp van \eqnref{speedupMetaheuristic} bepalen welke versnelling mogelijk is. Dit is mogelijk wanneer $\sigma<\lambda$. Een opmerkelijk feit is dat deze grenzen superlineaire speed-up niet uitsluiten. Sterker nog, \cite{DBLP:journals/jc/ShonkwilerV94} ontwikkelen in hun paper enkele metaheuristieken voor problemen waar men ook effectief superlineaire speed-up kan vaststellen.

\paragraph{}

Superlineaire speed-up zorgt echter in de literatuur voor veel ophef\cite{superlinearSpeedup}. Theoretisch kan men immers een parallel proces op een sequenti\"ele processor emuleren. Deze processor voert dan afwisselend het werk uit van \'e\'en van de kernen. In de praktijk neemt men soms superlineaire speed-up waar en dit zelfs op een consistente manier. Doorgaans wijt men dit echter aan het \emph{cache-effect}: als we een algoritme op verschillende processoren uitvoeren beschikken we ook over meer snelle geheugens (\emph{cache}). Bovendien kan een deel van het verwerken van data soms uitbesteed worden aan de netwerkapparatuur (bijvoorbeeld bepalen welke processoren op de hoogte moeten worden gebracht van een bepaalde gebeurtenis). Vermits we echter in bovenstaande redenering abstractie van deze extra bronnen van speed-up kan dit de oorzaak niet zijn.

\paragraph{}

De anomalie zit dan ook waarschijnlijk in het feit dat de parallelle variant van de Monte-Carlo simulatie geen zuivere metaheuristiek is: wanneer we immers het aantal processoren opdrijven, drijven we ook het aantal initi\"ele populaties op. Zoals we uit \eqnref{sigmaMeaning} kunnen afleiden wordt $\sigma$ klein indien de beginverdeling $\vec{\hat{\alpha}}$ klein is of sterk afwijkt van $\evr[\hat{P}]$. In dat geval loont het dus eerder de moeite van telkens nieuwe populaties te genereren dan deze populaties te laten evolueren. We kunnen dus stellen dat in het geval $\sigma<\lambda$, de metaheuristiek eenvoudigweg slecht is opgesteld. De voorbeelden die \cite{DBLP:journals/jc/ShonkwilerV94} aanhaalt zijn dan ook problemen in \comp{P} of de metaheuristieken zijn niet correct afgesteld (bijvoorbeeld te lokaal) op het probleem.

\subsubsection{Minimale Speed-up}

Metaheuristieken worden ge\"implementeerd als een evoluerende populatie (mogelijk bestaat de populatie uit \'e\'en individu) met transitiefuncties. De reden is dat men verwacht dat men iets uit het verleden kan leren en goede oplossingen meestal minimaal van elkaar verschillen (dit kan beargumenteerd worden met een studie in \cite{kirkPatrick}). Indien we aan deze assumptie geen geloof hechten kunnen we op een andere manier de optimale oplossing proberen te vinden. Vermits de variabelen en de bijbehorende domeinen op voorhand gekend zijn, kan een programma een toevallige oplossing genereren. Bovendien kunnen we een programma zo ontwerpen dat het uitsluitend oplossingen genereert die nog niet eerder gegenereerd werden in dit in een tijdscomplexiteit onafhankelijk van het aantal eerder gegenereerde oplossingen.
\paragraph{}
Stel dat het domein exact \'e\'en optimale oplossing bevat, dan is de kans dat we in iteratie $t$ deze waarde vinden gelijk aan:
\begin{equation}
\Prob{\theta=t|\theta\geq t}=\guards{
0&\mbox{if }t>N\\
1/N-t+1&\mbox{otherwise}
}
\end{equation}
Met $N$ het aantal mogelijke configuraties ($N=\bigoh{\prod_i\abs{A_i}}$). In dat geval is gemiddelde raaktijd:
\begin{equation}
\mean{\hittime}=\displaystyle\sum_{i=1}^N\Prob{\hittime\geq i}=\displaystyle\sum_{i=1}^N\displaystyle\frac{i}{N}=\displaystyle\frac{N+1}{2}
\end{equation}
In het geval dat we dit algoritme op $p$ machines laten draaien bekomen we:
\begin{equation}
\mean{\phittime}=\displaystyle\sum_{i=1}^N\Prob{\phittime\geq i}=\displaystyle\sum_{i=1}^N\Prob{\hittime\geq i}^p=\displaystyle\frac{1}{N^p}\displaystyle\sum_{i=1}^Ni^p\approx\displaystyle\frac{N}{p+1}
\end{equation}
De laatste benadering bekomen we omdat de som een veelterm van graad $p+1$ is met $1/p+1$ als leidende co\"effici\"ent. Bijgevolg is de \emph{speed-up} bij benadering gelijk aan:
\begin{equation}
\mbox{speed-up}=\displaystyle\frac{\mean{\hittime}}{\mean{\phittime}}\approx\displaystyle\frac{\brak{N+1}\cdot\brak{p+1}}{2\cdot N}\approx\displaystyle\frac{p+1}{2}
\eqnlab{minimalSpeedupMetaheuristic}
\end{equation}
De formule is verder te veralgemenen naar $g$ gewenste oplossingen maar leidt tot dezelfde speed-up.

\subsubsection{A flaw in the plan: anomalie\"en in het model}



\subsection{Parallelliseren van metaheuristieken in de praktijk}

Een referentiewerk hoe men metaheuristieken in de praktijk op verschillende processoren laat werken is ``\emph{Parallel Metaheuristics: A New Class of Algorithms}'' van Alba\cite{Alba2005book}. In het boek maakt men het eerder vermelde onderscheid tussen \emph{Local Search Metaheuristics (LMS)} en anderzijds \emph{Evolutionary Algorithms (EA)}.
\paragraph{}
Bij de \emph{Local Search Metaheuristics} ziet men drie bronnen tot parallellisme:
\begin{enumerate}
 \item \emph{Parallel multistart}: de processoren starten met een verschillende initi\"ele populatie en werken min of meer onafhankelijk. Dit is het parallellisatie-paradigma die we in de modellering hebben beschouwd.
 \item \emph{Parallel moves}: we beschouwen \'e\'en globale actieve oplossing. Wanneer we een migratie uitvoeren op een sequenti\"ele implementatie genereren we echter \'e\'en oplossing. We kunnen elke processor een migratie laten uitvoeren en vervolgens uit de verzameling van oplossingen uit de omgeving een nieuwe actieve oplossing kiezen. In het geval we een \emph{Local Search} stap uitvoeren kunnen we bijvoorbeeld ook elke processor een deel van de volledige omgeving laten afzoeken.
 \item \emph{Move acceleration}: een migratie kan er soms uit bestaan dat een significant deel van de oplossing verandert. Bovendien kan de evaluatiefunctie zelf ook moeilijk uit te rekenen zijn. Bij \emph{move acceleration} zijn we in staat om de oplossing op te delen in min of meer onafhankelijke delen. Elke processor past een deel van de configuratie aan en rekent de verandering van fitness-waarde met betrekking tot dit deel uit. Soms is er nog postprocessing vereist om de uiteindelijke fitness-waarde te berekenen.
\end{enumerate}

\paragraph{}
In het geval van \emph{Evolutionary Algorithms} ziet men drie bronnen tot parallellisme:
\begin{enumerate}
 \item \emph{Parallel computation}: meestal dient men operaties toe te passen op een significant deel van de populatie (bijvoorbeeld het berekenen van fitness-waardes). Vermits oplossingen nog steeds onafhankelijke entiteiten zijn, is dit een inherente vorm van parallellisatie.
 \item \emph{Parallel population}: men kan ook op elke processor en onafhankelijke populatie beschouwen waarop de processor dan werkt. Meestal gaat dit enigszins gepaard met kruisbestuiving: het uitwisselen van sterke oplossingen. Meestal noemt een genetisch algoritme waarbij men tussenschotten tussen populaties plaatst een \emph{Island Model}.
\end{enumerate}

\begin{algorithm}[H]
 \SetAlgoLined
 %\KwData{}
 %\KwResult{how to write algorithm with \LaTeX2e }
 \ParFor($i=1\ldots p$){
  Bereken een initi\"ele set van oplossingen $\PopSet_{1,i}$ en hun fitness-waarde\;
  $b_{1,i}\leftarrow\displaystyle\argmin_{\sol\in\PopSet_{1,i}}{\fun{\evalfun}{\sol}}$\;
 }
 $B_1\leftarrow\displaystyle\argmin_{\sol\in\accl{b_{1,1},b_{1,2},\ldots,b_{1,p}}}{\fun{\evalfun}{\sol}}$\;
 $t\leftarrow1$\;
 \Repeat{het stopcriterium is bereikt}{
   \ParFor($i=1\ldots p$){
	Genereer op basis van $\tupl{\PopSet_{t,1},\PopSet_{t,2},\ldots,\PopSet_{t,p}}$ en $t$ stochastisch een nieuwe set oplossingen $\PopSet_{t+1,i}$ samen met hun fitness-waarde\;
	$b_{t+1,i}\leftarrow\displaystyle\argmin_{\sol\in\PopSet_{t+1,i}\cup\accl{b_{t,i}}}{\fun{\evalfun}{\sol}}$\;
   }
   $B_{t+1}\leftarrow\displaystyle\argmin_{\sol\in\accl{b_{t+1,1},b_{t+1,2},\ldots,b_{t+1,p}}}{\fun{\evalfun}{\sol}}$\;
   $t\leftarrow t+1$\;
 }
 \KwRet{$B_t$}
 \caption{Hoog niveau beschrijving van een parallelle metaheuristiek.}
 \alglab{parallelMetaheuristicGeneral}
\end{algorithm}

% \begin{figure}
% \centering
% \begin{tikzpicture}
% \foreach \a/\x/\y/\t in {0/0/2/Doel,1/5/0/Niet-doel,2/7/0/Initieel,3/9/0/Limiet} {
%   \node (L\a) at (\x,-\y+0.5) {\t};
% }
% \foreach \x/\t in {2/1,3/2,5/g} {
%   \node[draw=black,minimum width=1.5 cm,rectangle] (G\x) at (0,-\x) {$\PopSet_{\t}$};
% }
% \node[draw=black,dashed,fit=(L0) (G2) (G5),minimum width=1.5 cm,inner sep=10pt,rectangle] (GA) {};
% \foreach \x/\t/\u in {0/g+1/1,1/g+2/2,2/g+3/3,4/i/i-g,6/j/j-g,8/N+g/N} {
%   \node[draw=black,minimum width=1.5 cm,rectangle] (P\x) at (5,-\x) {$\PopSet_{\t}$};
%   \node (IP\x) at (P\x -| L2) {$\hat{\alpha}_{\u}$};
%   \node (EP\x) at (P\x -| L3) {$w_{\u}$};
%   \draw[->] (P\x.west) to node[above,midway,sloped]{$1-\hat{v}_{\u}$} (GA);
% }
% \node[draw=black,dashed,fit= (L1) (L3) (P0) (P8),minimum width=1.5 cm,inner sep=10pt,rectangle] (PA) {};
% \coordinate (LC) at ($(PA.south)+(0,-0.25)$);
% \coordinate (TR) at ($(PA.north east)+(0.25,0.25)$);
% \draw[->] (PA.south) -- ++(0,-0.25) to node[above,midway,sloped]{$1-\lambda$} (GA.south |- LC) -- (GA.south);
% \draw[->] (PA.north) -- ++(0,0.25) to node[above,midway,sloped]{$\lambda$} (TR) |- (PA.east);
% \end{tikzpicture}
% \caption{Schematische voorstelling van het effect van de parameters}
% \end{figure}

% \begin{figure}
% \begin{tikzpicture}[scale=4,domain=0.1:0.9]
% \draw[very thin,color=gray] (-0.1,-0.1) grid (1.1,1.1);
% \draw[->] (-0.2,0) -- (1.2,0) node[right] {$\lambda$};
% \draw[->] (0,-0.2) -- (0,1.2) node[above] {$\mean{\theta}$};
% \foreach \s/\t in {0.1/,0.05/dashed,0.025/dotted} {
%   \draw[\t] plot[id=x] function{\s/(x*(1-x))} node[right] {$\sigma=\s$};
% }
% \end{tikzpicture}
% \caption{Invloed van de parameter $\lambda$ op de raaktijd}
% \end{figure}
\section{Besluit van dit hoofdstuk}
Als je in dit hoofdstuk tot belangrijke resultaten of besluiten gekomen
bent, dan is het ook logisch om het hoofdstuk af te ronden met een
overzicht ervan. Voor hoofdstukken zoals de inleiding en het
literatuuroverzicht is dit niet strikt nodig.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "masterproef"
%%% End:
