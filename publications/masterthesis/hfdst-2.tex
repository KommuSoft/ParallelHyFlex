\chapter{Studie naar Sequenti\"ele Hyperheuristieken (CHeSC2011)}
\label{hoofdstuk:2}

Alvorens zelf hyperheuristieken te implementeren, is het interessant om te analyseren aan welke eigenschappen een goede hyperheuristiek moet voldoen. We verwachten immers dat bij het effici\"ent paralleliseren van deze algoritmen sommige wetmatigheden kunnen worden overgenomen. Vandaar deze studie naar \abseqe{} \abhhn{}.

\section{Implementatie-set}

\subsection{\abhf{}}

In 2010 publiceren \auth[5586064]{Burke et al.} een paper waarin ze \abhf{} voorstellen. \abhf{} is een klassenbibliotheek geschreven in \abjava{}. Het laat toe dat de gebruiker hyperheuristieken implementeert zonder dat het algoritme de details kent van het onderliggende probleem en de \abllhn{}. Het systeem werkt op basis van geheugen: een lijst waarin tussentijdse oplossingen worden opgeslagen en uitgelezen. Het systeem biedt vervolgens de mogelijkheid om een \abllh{} met een specifieke index toe te passen op de oplossing op een specifieke plaats in het geheugen en het resultaat op een specifieke plaats op te slaan. Daarnaast kunnen gebruikers ook de objectiviteitswaarde van een bepaalde oplossing in het geheugen opvragen en vragen of twee oplossingen in het geheugen equivalent zijn.

\paragraph{}
Het programma heeft dan wel geen idee wat de onderliggende \abllhn{} precies doen, ze worden wel gecategoriseerd in 4 verschillende types:
\begin{enumerate}
 \item \emph{\abmt{}}: hierbij wordt de oplossing op een toevallig manier aangepast. Dit soort heuristieken heeft geen componenten die inschatten of de verandering onmiddellijk of op termijn tot betere resultaten zal leiden.
 \item \emph{\abco{}}: dit zijn de enige heuristieken die twee oplossingen recombineren in een nieuwe oplossing. Het is uiteraard de bedoeling dat de nieuwe oplossing karakteristieken gemeen heeft met beide ``ouders''.
 \item \emph{\abrr{}}: deze heuristieken breken een deel van de oplossing af, om ze dan vervolgens met behulp van bijvoorbeeld een gretig algoritme terug op te bouwen.
 \item \emph{\abls{}}: dit is een familie van algoritmen die herhaaldelijk mutaties uitvoeren indien deze mutaties ook per stap winst opleveren. Indien geen enkele mutatie meer tot een beter resultaat leidt stopt het algoritme.
\end{enumerate}
\abllhn[L]{} die tot de types \abmt{}, \abco{} en \abrr{} behoren worden ook wel \abpts{} genoemd. Het softwaresysteem houdt ook de mogelijkheid open om een \abllh{} te classificeren onder ``other'', maar voor zover ons bekend zijn er nog geen heuristieken in \abhf{} geschreven die onder deze categorie vallen. Verder kan er verwarring optreden over het feitelijke verschil tussen \abrr{} en \abls{}. We kunnen immers bij beide families verwachten dat ze minstens een oplossing opleveren die beter is dan het origineel (\abrr{} zal immers tot in dat geval het afgebroken gedeelte reconstrueren zoals het origineel). Een verschil die men doorgaans maakt is dat \abls{} een operator is die idempotent is.

\paragraph{}
\importtikz[1.4]{hyflexstructure}{parhyflexstructure}{Schematische voorstelling van \emph{HyFlex}.}


\subsection{\abchescy{}}

Om meer aandacht te vestigen op \abhf{} werd in 2011 een wedstrijd georganiseerd door de universiteit van Nottingham: de ``\emph{Cross-domain Heuristic Search Challenge (\abchescy)}''\cite{Burke:2011:CHS:2177360.2177415}. De verschillende programma's krijgen een set van verschillende problemen en worden gequoteerd op basis van de kwaliteit van de oplossingen die ze na 10 minuten uitvoer afleveren.%TODO(check)
De wedstrijd omvatte problemen uit zes verschillende domeinen: \prob{Maximal Satisfiability}, \prob{Bin Packing}, \prob{Personnel Scheduling}, \prob{Flow Shop}, \prob{Travelling Salesman Problem} en het \prob{Vehicle Routing Problem}.

\paragraph{}
In totaal telde de competitie 20 teams. We hebben met onze studie de zestien implementaties die gedocumenteerd werden bestudeerd. Tabel \ref{tbl:chescParticipants} bevat een lijst met de verschillende implementaties en geeft aan welke systemen in de studie opgenomen werden.

\begin{table}[hbt]
  \centering
  \begin{tabular}{rllrc} \toprule
    \#&Naam&Auteur/Team&Score&Bestudeerd\\\midrule
    1&	\emph{AdapHH}\cite{chesc-adaphh,chesc-adaphh2,348072}	&	Mustafa M\i{}s\i{}r&	181.00&	$\checkmark$\\
    2&	\emph{VNS-TW}\cite{chesc-vns-tw}&				Mathieu Larose&		134.00&	$\checkmark$\\
    3&	\emph{ML}\cite{chesc-ml,chesc-ml2}&				Mustafa M\i{}s\i{}r&	131.50&	$\checkmark$\\
    4&	\emph{PHUNTER}\cite{chesc-phunter}&				Fan Xue&		93.25&	$\checkmark$\\
    5&	\emph{EPH}\cite{chesc-eph}&					David Meignan&		89.75&	$\checkmark$\\
    6&	\emph{HAHA}&							Andreas Lehrbaum&	75.75&	\\
    7&	\emph{NAHH}&							MFranco Mascia&		75.00&	\\
    8&	\emph{ISEA}\cite{chesc-isea}&					Jiri Kubalik&		71.00&	$\checkmark$\\
    9&	\emph{KSATS-HH}\cite{chesc-ksats-hh}&				Kevin Sim&		66.50&	$\checkmark$\\
    10&	\emph{HAEA}\cite{chesc-haea,Gomez04selfadaptation}&		Jonatan Gomez&		53.50&	$\checkmark$\\
    11&	\emph{ACO-HH}\cite{chesc-aco-hh}&				Jos\'e Luis N\'u\~nez&	39.00&	$\checkmark$\\
    12&	\emph{GenHive}\cite{chesc-genhive}&				CS-PUT&			36.50&	$\checkmark$\\
    13&	\emph{DynILS}\cite{chesc-dynils,journals/orsnz/ksosils}&	Mark Johnston&		27.00&	$\checkmark$\\
    14&	\emph{SA-ILS}&							He Jiang&		24.25&	\\
    15&	\emph{XCJ}&							Kamran Shafi&		22.50&	\\
    16&	\emph{AVEG-Nep}\cite{chesc-aveg-nep}&				Thommaso Urli&		21.00&	$\checkmark$\\
    17&	\emph{GISS}\cite{chesc-giss}&					Alberto Acu\~na&	16.75&	$\checkmark$\\
    18&	\emph{SelfSearch}\cite{chesc-selfsearch}&			Jawad Elomari&		7.00&	$\checkmark$\\
    19&	\emph{MCHH-S}\cite{chesc-mchh-s,conf/gecco/McClymontK11}&	Kent McClymont&		4.75&	$\checkmark$\\
    20&	\emph{Ant-Q}\cite{chesc-ant-q,sis/ant-q}&			Imen Khamassi&		0.00&	$\checkmark$\\
    \bottomrule
  \end{tabular}
  \caption{Deelnemers van de \abchescy{} competitie\cite{chesc-results}.}
  \label{tbl:chescParticipants}
\end{table}

we zullen de bestudeerde implementaties kort bespreken en vervolgens enkele hypotheses aanbrengen waaraan goed presterende hyperheuristieken waarschijnlijk dienen te voldoen.

\section{Implementaties}

\subsection{\emph{Ant-Q} (\#20)}
\label{sss:ant-q}
\subsubsection{Implementatie}
\emph{Ant-Q}\cite{chesc-ant-q,sis/ant-q} combineert ``\emph{ant-computing}''\cite{Michael:2009:AC:1596832.1596835} met ``\emph{Q-learning}''\cite{citeulike:5925674}. Dit doet men door een graaf te beschouwen waar de metaheuristieken de knopen voorstellen. De bogen bevatten een niet genormaliseerde kans om deze boog te nemen. Vervolgens voert men op een populatie oplossingen metaheuristieken uit door van knoop naar knoop te bewegen. De volgende knoop wordt gekozen op basis van een kansverdeling volgens de bogen die verbonden zijn met de oorspronkelijke knoop. Nadat de heuristiek is toegepast, wordt de kansverdeling van alle bogen die de oplossing tot dusver heeft gevolgd aangepast. De oplossing die op dat moment de beste is beloont alle bogen die hij gepasseerd heeft. Door de waarde van de bogen aan te passen zullen andere oplossingen meer geneigd zijn om een gelijkaardig pad te kiezen.
\subsubsection{Kritiek}
\begin{itemize}
 \item Het volledige pad van de winnende oplossing krijgt een bonus (ook bogen die hier helemaal niet te hebben bijgedragen)
 \item Wanneer de beste oplossing lange tijd hetzelfde is, krijgen de relevante bogen een grote bonus, hierdoor zit er na verloop van tijd nog weinig creativiteit in het systeem.
 \item Het type metaheuristiek speelt geen rol in het algoritme. Hierdoor is er een grote kans dat \abls{} heuristieken na verloop van tijd vaak worden toegepast. Dit leidt bovendien tot een stabiel systeem: \abls{} heuristieken zijn immers idempotent waardoor de beste oplossing dezelfde zal blijven.
\end{itemize}
\subsection{\emph{MCHH-S: Markov Chain Hyper-Heuristic} (\#19)}
\label{sss:mchh-s}
\subsubsection{Implementatie}
\emph{MCHH-S}\cite{chesc-mchh-s,conf/gecco/McClymontK11} werkt op een gelijkaardig manier aan Ant-Q (zie \ref{sss:ant-q}): men ontwerpt een graaf waar de knopen de metaheuristieken voorstellen en de bogen overgangen die men met een kans labelt. Het algoritme verschilt echter omdat er slechts \'e\'en oplossing in het netwerk rondwandelt. Daarnaast worden de kansen ook op een andere manier berekend: enkel de laatste boog verandert op basis van de onmiddellijke verandering van de fitness-waarde van de oplossing. Het algoritme verschilt ook omdat het niet telkens de nieuwe oplossing accepteert: alleen indien de oplossing beter is, of probabilistisch volgens het aantal opeenvolgende iteraties dat er nog geen betere oplossing werd gevonden.
\subsubsection{Kritiek}
\begin{itemize}
 \item Dit algoritme kan in een lokaal optimum terecht komen vermits \abls{} doorgaans voor de grootste winst zorgen en dus vaker beloond zullen worden. De idempotentie van \abls{} heuristieken zorgt er echter voor dat we veel rekenkracht verliezen met het herhaaldelijk toepassen van \abls{} heuristieken.
 \item Men maakt geen onderscheid tussen de verschillende types heuristieken: mutatie zal meestal tot een tijdelijk slechtere oplossing leiden. De bogen naar mutaties bestraffen is echter waarschijnlijk niet wenselijk. (net als bij \emph{Ant-Q}, zie \ref{sss:ant-q}).
\end{itemize}
\subsection{\emph{SelfSearch} (\#18)}
\label{sss:selfsearch}
\subsubsection{Implementatie}
\emph{SelfSearch}\cite{chesc-selfsearch} werkt met een populatie van oplossingen. Tijdens elke iteratie kiest men een \abllh{} die men vervolgens op alle oplossingen toepast. Hierdoor verdubbelt de populatiegrootte. Om terug op de originele populatiegrootte uit te komen selecteert men de beste unieke oplossingen. De keuze van de heuristiek gebeurt probabilistisch en op basis van twee strategie\"en: exploratie en exploitatie. Bij de exploratie krijgen heuristieken die een resultaat opleveren die verschilt van het origineel meer gewicht. Bij de exploitatie vooral heuristieken die in het verleden tot verbetering leidden.
\subsubsection{Kritiek}
\begin{itemize}
 \item Heeft de neiging in een lokaal optimum vast te raken: indien de populatie in een lokaal optimum zit, kan geen enkele mutatie die een tijdelijk slechtere oplossing levert de populatie terug uit dit lokaal optimum halen. De kans dat een volledige populatie in een lokaal optimum zit is uiteraard klein, maar desalniettemin kan dit algoritme resulteren in het toepassen van veel zinloze \abllh{}.
 \item Metaheuristieken die in het begin slecht presteren hebben meestal weinig kans op herintroductie: vermits ze na de initi\"ele fase minder gekozen worden, kunnen frequenter gekozen heuristieken een buffer van probabilistisch gewicht opbouwen.
 \item Er is slecht \'e\'en migratie van de exploitatie-fase naar de exploratie-fase. Indien deze fase op een fout moment gekozen wordt, is er geen weg terug.
\end{itemize}
\subsection{\emph{GISS: Generic Iterative Simulated Annealing Search} (\#17)}
\label{sss:giss}
\subsubsection{Implementatie}
\emph{GISS}\cite{chesc-giss} gebruikt \emph{Simulated Annealing}\cite{citeulike:1612433} als hyperheuristiek: bij elke iteratie kiest men uniform een beschikbare metaheuristiek die men toepast op het probleem. Vervolgens accepteert men deze oplossing volgens de procedure van simuated annealing. Crossover metaheuristieken worden toegepast op de laatste en voorlaatste oplossing. Indien er lange tijd geen verbetering zichtbaar is, wordt het systeem herstart vanaf een toevalsoplossing.
\subsubsection{Kritiek}
\begin{itemize}
 \item Uniforme selectie van \abllhn{} is waarschijnlijk niet interessant. Sommige \abllhn{} zijn immers nagenoeg overal beter dan anderen.
 \item \abco{} heuristieken toepassen tussen de laatste en de voorlaatste oplossing levert meestal weinig op, vermits de laatste gegenereerd is door een \abllh{} toe te passen op de voorlaatste.
 \item Er is geen overdracht van zoekervaring bij een (mogelijke) herstart. We kunnen echter verwachten dat we ook uit vorige rondes nuttige informatie kunnen leren.
 \item Er wordt opnieuw geen onderscheid gemaakt tussen het type van de \abllhn{}. Hierdoor verliest men mogelijk veel rekenkracht aan nutteloze operaties.
\end{itemize}
\subsection{\emph{AVEG-Nep: Reinforcement Learning Approach} (\#16)}
\label{sss:aveg-nep}
\subsubsection{Implementatie}
\emph{AVEG-Nep}\cite{chesc-aveg-nep} is gebaseerd op ``\emph{Reinforcement learning}''\cite{rlaiacaml}. Bij een reinforcement learning algoritme hebben we vijf componenten nodig:
\begin{enumerate}
 \item Een \emph{set acties}: hier is een actie een type van \abllhn{} (\abmt{}, \abls{}, \abco{}, \abrr{}) verreikt met de waarde van de relevante parameters gekwantiseerd per interval van 0.2. Wanneer we een bepaalde actie kiezen, kiezen we een \abllh{} die tot het geselecteerde type behoort en voeren deze uit met de geselecteerde parameters.
 \item Een \emph{beloningsfunctie} ofwel ``\emph{reward function}'': het verschil in fitnesswaarde tussen de nieuwe oplossing en de oorspronkelijke oplossing gedeeld door de tijd die de heuristiek nodig had om de nieuwe oplossing te berekenen.
 \item Een \emph{toestandsvoorstelling}: hier is dit het gewogen gemiddelde van het verschil in fitness-waarde over de tijd voor (we berekenen dit via ``\emph{exponential smoothing}''\cite{Taylor2003a})
 \item Een \emph{beleid} ofwel ``\emph{policy}'': hiervoor gebruiken we de klassieke ``\emph{$\epsilon$-greedy}''-methode.
 \item Een \emph{Leerfunctie} ofwel ``\emph{learning function}'': ook hiervoor gebruiken we bij \emph{AVEG-Nep} een ``\emph{exponential smoothing}''-functie.
\end{enumerate}
In \emph{AVEG-Nep} laten we 4 verschillende agenten tegelijk laat werken op de het probleem. De rede is dat men bij het toepassen van een \abco{} \abh{} een andere oplossing nodig heeft. In dat geval kiest men de oplossing van een andere agent om een \abco{} mee te realiseren.
\subsubsection{Kritiek}
\begin{itemize}
 \item Geen onderscheid tussen metaheuristieken van dezelfde familie: terwijl de parameters een andere semantische betekenis kunnen hebben, en de \'en\'e heuristiek soms significant beter kan werken dan de andere.
 \item Problemen met de voorstelling van de toestand: naarmate het algoritme vordert verwachten we een minder sterke groei van de fitness-functie (zie \ref{sss:gedragmetaheuristieken}). Hierdoor komen we in niet-verkende toestanden waar we weinig ervaring hebben opgebouwd.
\end{itemize}
\subsection{\emph{DynILS: Dynamic Iterated Local Search} (\#13)}
\label{sss:dyn-ils}
\subsubsection{Implementatie}
\emph{Dynamic Iterated Local Search}\cite{chesc-dynils,journals/orsnz/ksosils} is een implementatie van ``\emph{Iterative Local Search}''\cite{Lourenco02iteratedlocal}. Men kiest telkens uniform een \abpte{} \abllh{} en past vervolgens alle \abls{} \abhn{} toe. Daarnaast introduceert met twee kleine wijzigingen: vermits de metaheuristieken parameters hebben, probeert het algoritme deze parameters te optimaliseren. Hiertoe houdt men een vector voor een aantal waardes van deze parameters. Met behulp van deze vector rekent men dan de kans uit dat de parameter op een bepaalde waarde wordt gezet. Indien een \abllh{} tot een betere oplossing komt verhoogt men de kans van de gekozen parameterwaarde. In het andere geval neemt de kans af. Een tweede aanpassing is de ``\emph{non-improvement bias}'': we verhogen de parameter evenredig met het aantal opeenvolgende iteraties waarin we de oplossing niet konden verbeteren. Door de parameter te verhogen zoeken we een groter gebied af met \abls{} en muteren we de oplossing ook sterker. Hierdoor hopen we uit een lokaal optimum te ontsnappen.
\subsubsection{Kritiek}
\begin{itemize}
 \item Selectie van de mutatie is vrij eenvoudig: er wordt geen moeite gedaan om een bepaalde kansverdeling te leren.
 \item Vaste volgorde bij het toepassen van \abls{}: hierdoor kan de ene \abls{} \abh{} de andere consistent blokkeren.
\end{itemize}
\subsection{\emph{GenHive: Genetic Hive Hyperheuristic} (\#12)}
\label{sss:genhive}
\subsubsection{Implementatie}
\emph{GenHive}\cite{chesc-genhive} is een hyperheuristiek die werkt op basis van een genetisch algoritme. Een individu in dit genetische algoritme is een sequentie van metaheuristieken die op het probleem worden toegepast. Enkele van deze individuen zijn actief: ze worden toegekend aan een oplossing in de zoekruimte en worden er bij een iteratie op toegepast. Nadien worden de resultaten beoordeelt. De beste strategie\"en blijven actief. De overige worden passieve individuen. Men dient echter aan elke oplossing een strategie toekennen. Hiertoe worden individuen die in de vorige iteratie passief waren gerecombineerd met de beste individuen en worden deze geactiveerd.
\subsubsection{Kritiek}
\begin{itemize}
 \item Algoritme bevat veel parameters: populatiegrootte, aantal individuen actief, aantal individuen die na de iteratie behouden blijft,... Het vinden van een correcte configuratie is niet triviaal.
 \item De beste individuen blijven aan dezelfde oplossingen gelinkt, terwijl men deze zou kunnen gebruiken om andere oplossingen ook significant te verbeteren. Bovendien verwachten we dat een strategie de tweede maal minder effectief zal zijn.
 \item Redundante aspecten in een oplossing (een sequentie oproepen die nooit een beter resultaat kunnen genereren) wordt niet ge\"elimineerd: hierdoor is men niet effici\"ent met tijdsgebruik.
 \item Het is ineffici\"ent om altijd alle oplossingen verder te ontwikkelen in een iteratie.
\end{itemize}
\subsection{\emph{ACO-HH: Ant Colony Optimization} (\#11)}
\label{sss:aco-hh}
\subsubsection{Implementatie}
\emph{ACO-HH}\cite{chesc-aco-hh} maakt gebruik van de ``\emph{Ant-Colony Optimization}''\cite{hom/aco} techniek. Net als bij Ant-Q (zie \ref{sss:ant-q}) beschouwen we een graaf waarbij knopen de \abllhn{} voorstellen. Een fundamenteel verschil is echter dat de graaf voorgesteld wordt als een tabel met $n$ kolommen en $H$ rijen (met $H$ het aantal metaheuristieken en $n$ een parameter genaamd de \emph{padlengte}). Elke \abllh{} komt in deze graaf juist $n$ keer voor. Verder beschouwen we enkel bogen tussen twee verschillende kolommen. Het is de bedoeling dat agenten een pad afleggen met padlengte $n$ en telkens bij een knoop de overeenkomstige \abllh{} toepassen. Elke agent start met dezelfde initi\"ele oplossing en legt een pad van $n$ stappen af. De volgende knoop wordt telkens gekozen op basis van labels op de bogen die een niet-genormaliseerde kans aangeven. Wanneer alle agenten de laatste kolom bereikt hebben worden de resultaten ge\"evalueerd. Zo worden alle bogen aangepast waarover een agent heeft gelopen op basis van het verschil in fitnesswaarde van de oorspronkelijke en uiteindelijke oplossing. Uit de set van finale oplossingen dient men ook een oplossing te kiezen die de initi\"ele oplossing van de volgende fase voorstelt. Doorgaans neemt men de beste oplossing over alle fases heen, tenzij de huidige fase deze oplossing heeft gegenereerd.
\subsubsection{Kritiek}
\begin{itemize}
 \item Het systeem kan in een lange tijd in een lokaal optimum terechtkomen: sommige paden kunnen in het begin tot sterke kansen komen, waardoor het moeilijk is om na verloop van tijd een sterker pad te belonen.
 \item Redundante aspecten in een sequentie worden niet noodzakelijk ge\"elimineerd: elke pad is bijvoorbeeld even lang en zolang een pad van dezelfde lengte niet met sterkere resultaten komt is er geen reden kunnen er ineffici\"ente heuristieken in het meest populaire pad zitten. Men zou in de graaf bijvoorbeeld ook \emph{nul-operaties} kunnen voorstellen. Een agent die in zo'n operatie terecht komt voert nadien geen operaties meer uit tot het einde van de iteratie. Men dient dan ook paden die zo'n nul-operatie introduceren extra te belonen.
\end{itemize}
\subsection{\emph{HAEA: Hybrid Adaptive Evolutionary Algorithm} (\#10)}
\label{sss:haea}
\subsubsection{Implementatie}
\emph{HAEA}\cite{chesc-haea,Gomez04selfadaptation} is een hyperheuristiek die werkt op basis van drie oplossingen: de \emph{ouder}, het \emph{kind} (een oplossing gegenereerd uit de ouder) en de \emph{beste oplossing} tot dan toe. Daarnaast houdt men twee verzamelingen bij van heuristieken: \texttt{heu} houdt vier heuristieken bij uit de lijst van alle \abllhn{}. \texttt{loc} houdt vier heuristieken bij uit de lijst van \abls{} \abllhn{}. De heuristieken die in deze verzamelingen zitten zijn ``\emph{in use}''. Dat betekent dat bij iedere iteratie we een algemene heuristiek kiezen uit \texttt{heu} en een \abls{} \abllh{} uit \texttt{loc}. Vervolgens passen we deze heuristieken na elkaar toe op  de ouder. Bij een \abco{} \abllh{} combineren we de ouder met de beste oplossing tot dusver. Indien dit kind beter is dan de ouder accepteren we het kind als nieuwe ouder en belonen we beide heuristieken. Dit doen we door de kans te verhogen dat ze de volgende maal opnieuw gekozen worden. Indien het kind niet beter presteert verlagen we de kans dat de heuristieken nogmaals gekozen worden. Indien na enkele iteraties er nog steeds geen verbetering is, kiezen we nieuwe elementen voor \texttt{heu} en \texttt{loc}.
\subsubsection{Kritiek}
\begin{itemize}
 \item \abco[C]{} wordt niet optimaal gebruikt: de kans is groot dat de huidige oplossing al dicht bij de beste oplossing zit.
 \item Beide metaheuristieken worden beloond terwijl de oorzaak van de verbetering eerder te wijten kan zijn door het combineren van de twee configuraties.
 \item Bij het kiezen van nieuwe sets wordt de opgedane kennis over metaheuristieken tenietgedaan.
\end{itemize}
\subsection{\emph{KSATS-HH: Simulated Annealing with Tabu Search} (\#9)}
\label{sss:ksats-hh}
\subsubsection{Implementatie}
\emph{KSATS-HH}\cite{chesc-ksats-hh} is een implementatie die gebaseerd is op ``\emph{Simulated Annealing}''\cite{citeulike:1612433}: er is sprake van \'e\'en actieve oplossing. Na het toepassen van een \abllh{} accepteert men het resultaat van de oplossing indien de oplossing beter is, of met een bepaalde kans (die exponentieel daalt naarmate het resultaat veel slechter is) een slechtere oplossing accepteert. Een intelligent aspect hierbij is dat men op basis van ervaring uit het verleden gebruikt om het verschil in fitness-waarde van de oplossingen eerst te normaliseren (het verschil kan immers afhankelijk zijn van het probleem domein of de instantie). Het koelingsschema werkt ook exponentieel maar de factor waarmee men vermenigvuldigt verschilt in iedere tijdstap en hangt af van de het aantal iteraties die men in \'e\'en tijdseenheid weet te realiseren. De keuze van de heuristiek die wordt toegepast werkt op basis van ``\emph{Tabu Search}''\cite{DBLP:journals/informs/Glover89}: het algoritme houdt een lijst bij van heuristieken. Heuristieken die erin slagen om de oplossing te verbeteren stijgen in de lijst. Heuristieken die daar niet in slagen dalen een plaats en worden ``\emph{tabu}'' voor de volgende 7 iteraties. De uiteindelijke selectie van de heuristiek gebeurt door twee heuristieken uit de lijst te selecteren die niet ``\emph{tabu}'' zijn. De heuristiek met die het hoogst in de lijst gegenereerde lijst staat wordt dan gekozen.
\subsubsection{Kritiek}
\begin{itemize}
 \item Het systeem bestraft heuristieken voordat het algoritme begint: er zit een inherente orde in de lijst. Het element die als laatste geclassificeerd staat kan per toeval net de best presterende heuristiek zijn. Het kan lang duren voor deze heuristieken toch worden geselecteerd (vermits in het begin tot een betere oplossing komen vrij eenvoudig is).
\end{itemize}
\subsection{\emph{ISEA: Iterated Search by Evolutionary Algorithm} (\#8)}
\label{sss:isea}
\subsubsection{Implementatie}
\emph{Iterated Search by Evolutionary Algorithm}\cite{chesc-isea} is een algoritme die gebaseerd is op het eerder gepubliceerde \emph{POEMS}\cite{eurogp06:KubalikFaigl}. Hierbij verdeelt men de tijd op in een sequentie van ``\emph{epochs}''. In zo'n \emph{epoch} voert men een evolutief algoritme uit op prototypes. Een prototype is een sequentie van een variabel aantal metaheuristieken. Bij elk van deze metaheuristieken zijn ook de parameters reeds vastgezet. Tijdens een epoch wordt in een vast aantal iteraties een populatie van prototypes door een genetisch algoritme geoptimaliseerd. Nadien wordt het beste resultaat die met deze prototypes bereikt werd als nieuwe actieve oplossing gebruikt in de volgende epoch. 
\subsubsection{Kritiek}
\begin{itemize}
 \item Redundante aspecten in een sequentie worden niet noodzakelijk ge\"elimineerd: het algoritme is niet effici\"ent met tijdsgebruik.
 \item Na een epoch wordt de opgedane ervaring genegeerd. We verwachtten dat processortijd verloren zal gaan aan het opnieuw leren onderscheiden van goede en slechte prototypes.
\end{itemize}
\subsection{\emph{EPH: Evolutionary Programming Hyper-heuristic} (\#5)}
\label{sss:eph}
\subsubsection{Implementatie}
De \emph{Evolutionary Programming Hyper-Heuristic}\cite{chesc-eph} werkt op basis van twee populaties: een populatie oplossingen en een populatie van sequenties van metaheuristieken. Beide populaties evolueren tegelijk. De populatie oplossing bestaat uit $N$ oplossingen die ``random'' ge\"initialiseerd worden. Telkens bij de evaluatie van de populatie van de sequenties worden er nieuwe oplossingen gegenereerd. Telkens nadat zo'n sequentie is toegepast op een oplossing. Zal men proberen dit resultaat in de populatie proberen te injecteren. Deze injectie is succesvol indien het een betere fitness waarde heeft dan minstens \'e\'en oplossing in de populatie en de fitness-waarde nog niet voorkomt. Ter compensatie wordt de slechtste oplossing uit de populatie gehaald. Zo'n sequentie van \abllhn{} bestaat uit twee delen: een \abpt{}-gedeelte met een maximale lengte van 2, en een \abls{} gedeelte van variabele lengte. Naast de \abllhn{} die we toepassen bevat een sequentie ook informatie over de parameters. Het algoritme zet nog enkele extra beperkingen op sequenties: indien we twee \abpt{}-\abhn{} beschouwen dienen ze verschillend te zijn; een \abpt{} \abh{} kan enkel op de eerste plaats staan. Bij het \abls{} gedeelte wordt een heuristiek ofwel \'e\'enmaal ofwel volgens een ``\emph{Variable Neighborhood Descent schema}''\cite{hom/vns} uitgevoerd. De populatie van de sequenties wordt initieel random bevolkt en evolueert door middel van mutatie en selectie. Hiervoor worden vier types mutaties gebruikt die met uniforme kans worden gekozen: modificeren van de \abpt{}-parameters, modificeren van de \abls{}-parameters, verwijderen/toevoegen van een \abpt{}, permutatie van de \abls{} \abhn{}. Op elke sequentie wordt een \abmt{} toegepast. Daarna wordt via ``\emph{2-tournament selection}''\cite{Miller95geneticalgorithms} de populatie opnieuw gehalveerd: twee toevallig gekozen sequenties nemen het in enkele rondes tegen elkaar op. In een ronde worden ze op eenzelfde individu in de oplossingsverzameling toegepast. De sequentie die na de rondes het vaakst met de beste nieuwe oplossing komt, wordt geselecteerd in de nieuwe generatie van sequenties.
\subsubsection{Kritiek}
\begin{itemize}
 \item Redundante aspecten in een sequentie worden niet noodzakelijk ge\"elimineerd: het algoritme is niet effici\"ent met tijdsgebruik.
\end{itemize}
\subsection{\emph{PHUNTER: Pearl Hunter} (\#4)}
\label{sss:phunter}
\subsubsection{Implementatie}
\emph{Pearl Hunter}\cite{chesc-phunter} is gebaseerd op de jacht op parels en zeedieren. Dit proces kan het best uitgelegd worden als herhaalde diversificatie. Dit doen men door in de eerste plaats de metaheuristieken op te delen in twee categorie\"en: \emph{dives} die overeenkomen met de \abls{} \abllhn{} en \emph{surface moves} die de andere \abhn{} omvat. Daarnaast deelt men \emph{dives} verder op in \emph{snorkling} en \emph{deep dives}. \emph{Snorkling} houdt in dat men een \abls{} algoritme met lage \emph{depth of search} uitvoert en stopt op het moment dat men een betere oplossing ontdekt. \emph{Deep dives} daarentegen zoeken met een hoge \emph{depth of search} en stoppen enkel wanneer er geen verbetering meer waargenomen wordt. \emph{PHUNTER} combineert \emph{surface moves} en \emph{dives} in zogenaamde \emph{move-dive} iteraties. Bij zo'n \emph{move-dive} vertrekt men van enkele initi\"ele oplossingen. Vervolgens past men \emph{surface moves} toe en laat men de populatie groeien. Nadien ordent men de oplossingen. Enkel op interessante oplossingen wordt \emph{snorkling} toegepast. De beloftevolle oplossingen die uit deze \emph{snorkling}-fase komen, worden dan gebruikt als basis voor een \emph{deep dive}. Een \emph{deep dive} bestaat uit een sequentie van verschillende \abls{} \abllhn{}. Omdat de volgorde waarin deze heuristieken worden toegepast een belangrijke rol kan spelen worden verschillende volgordes parallel ge\"evalueerd. Na het \'e\'enmalig toepassen van zo'n sequentie worden de sequenties vergeleken. Enkel de beste sequentie wordt dan nog herhaaldelijk toegepast. Een laatste aspect die uniek is aan \emph{PHunter} is \emph{surface learning}: men evalueert omgevingen op basis van het aantal \abls{} iteraties dat het kost in de \emph{snorkling}-fase om de oplossing te verbeteren. Afhankelijk van het aantal iteraties wordt een omgeving gecategoriseerd als \emph{shallow water}, \emph{sea trench} of \emph{buoy in the water}. Op basis van de omgeving wordt een strategie bepaald voor de \emph{surface moves} in de volgende iteratie. De vier mogelijke strategie\"en zijn: \emph{average calls}, \emph{crossover emphasized}, \emph{crossover only} en \emph{online pruning}. De strategie wordt gekozen op basis van een \emph{decision tree} die werd opgesteld door \emph{WEKA} voor een bepaalde testcase. Een laatste aspect is het \emph{mission restart} principe: indien voor een lange tijd geen betere oplossing wordt gevonden of de oplossingen in de populatie zijn door \emph{surface moves} te gelijklopend, begint men weer men een nieuwe initi\"ele oplossing.
\subsubsection{Kritiek}
\begin{itemize}
 \item \emph{Decision tree} lost het probleem met de parameters niet op: parameters zitten in de \emph{decision tree} (opgesteld op basis van een aantal testen op historische data).
\end{itemize}
\subsection{\emph{ML: Mathieu Larose} (\#3)}
\label{sss:ml}
\subsubsection{Implementatie}
\emph{ML}\cite{chesc-ml} is gebaseerd op de \abmh{} ``\emph{Coalition Based Metaheuristic (CBM)}''\cite{chesc-ml2}. In het \emph{CBM} algoritme, worden verschillende \emph{agent}en gegroepeerd in een ``\emph{coalition}''. Agenten die tot dezelfde \emph{coalition} behoren, zullen vervolgens parallel de zoekruimte onderzoeken en onderweg ervaring uitwisselen over welke metaheuristieken moeten worden geselecteerd. Dit leerproces gebeurt aan de hand van ``\emph{reinforcement learning}''\cite{rlaiacaml}. Elke agent werkt verder met een ``\emph{Diversification-Intensification cycle}''. In de diversificatie stap passen we een \abmt{} of \abrr{} \abllh{} toe. In de intensificatie cyclus passen we meerdere \abls{} heuristieken toe totdat geen enkele \abls{} \abh{} de oplossing nog verder kan verbeteren. Daarna wordt beslist of de gevonden oplossing de nieuwe actieve oplossing van de agent wordt. Het \emph{reinforcement learning} component probeert om een zo goed mogelijke zoekstrategie te ontwikkelen voor iedere agent. Hiertoe dienen we toestanden op te stellen (we proberen immers een \abh{} te kiezen in een bepaalde toestand). Als toestand worden voorwaarden gebruikt over welke \abllhn{} al zijn toegepast. Vervolgens probeert het leeralgoritme een matrix op te stellen die met behulp van gewichten bepaalt in welke situatie welke \abh{} waarschijnlijk het voordeligste is. De keuze van de \abh{} wordt dan uiteindelijk gemaakt via een ``\emph{roulettewheel}''-procedure\cite{DBLP:journals/corr/abs-1109-3627} op basis van de toestand waarin we ons op een gegeven moment bevinden. Naast het leren op basis van eigen ervaring wordt ook ervaring opgedaan door de matrices van andere agenten. Hiervoor wordt aan \emph{mimetism learning}\cite{655072} gedaan: een agent probeert de matrix van een andere agent te imiteren door een lineaire interpolatie tussen de eigen matrix en de vreemde matrix te nemen. De matrix wordt aan de andere agenten doorgegeven indien de agent een nieuw globaal optimum bereikt.
\subsubsection{Kritiek}
\begin{itemize}
 \item \emph{Reinforcement learning} introduceert nieuwe parameters, die natuurlijk intelligent moeten gekozen worden. Voor dit probleem wordt geen oplossing aangereikt.
 \item De kwaliteit van een \abllh{} hangt doorgaans af van de regio waar de oplossing is gelokaliseerd. Dit wordt hier niet in rekening gebracht.
\end{itemize}
\subsection{\emph{VNS-TW: Variable Neighborhood Search-based} (\#2)}
\label{sss:vns-tw}
\subsubsection{Implementatie}
\emph{VNS-TW}\cite{chesc-vns-tw} is zoals de naam doet vermoeden gebaseerd op ``\emph{Variable Neighborhood Search}''\cite{hom/vns}. Het is een populatie-gebaseerd algoritme en werkt op basis van vier stappen: \emph{shaking}, \emph{local search}, \emph{tabu} en \emph{vervangen-selectie}. Elke iteratie passen we toe op slechts \'e\'en oplossing. Aan het einde van de iteratie kunnen we eventueel van actieve oplossing veranderen. Bij stap 1 -- \emph{shaking} -- gebruikt men een toevallig gekozen heuristiek uit de set van \abmt{} of \abrr{} \abllhn{} en past men deze toe op de actieve oplossing. Vervolgens zullen we in de \emph{local search} fase een \abls{} \abh{} kiezen en deze toepassen op het resultaat van de \emph{shaking}-fase. De heuristiek wordt gekozen op basis van rang: eerst kiezen uit een set heuristieken die nog niet gekozen zijn of een beter resultaat opleverden (rang 1), daarna uit een reeks heuristieken die de vorige keer een gelijkwaardige (maar niet gelijke) oplossing opleverden (rang 0). Indien geen enkele heuristiek meer aan deze  voorwaarden voldoet, of we stellen na $c$ opeenvolgende pogingen geen verbetering vast, stoppen we de \emph{local search}-fase. In de volgende stap -- \emph{tabu} -- zullen we op basis van de afgeleverde oplossing, de \emph{shaking} heuristieken aanpassen. Dit doen we met behulp van ``\emph{Tabu Search}''\cite{DBLP:journals/informs/Glover89}: indien het eindresultaat slechter is dan het origineel komt de heuristiek in de \emph{tabu}-lijst terecht. Indien we tot een gelijkaardig resultaat komen doen we dit in 20\% van de gevallen. Tot slot passen we de populatie aan: indien het resultaat beter is dan het origineel neemt dit zijn plaats in in de populatie. Indien het resultaat slechter is, zal de slechte oplossing in de populatie plaats maken voor dit resultaat. We kiezen de nieuwe actieve oplossing met behulp van ``\emph{2-tournament selection}''\cite{Miller95geneticalgorithms} uit de populatie.
\subsubsection{Kritiek}
\begin{itemize}
 \item Er bestaat een kleine kans dat het algoritme vastloopt op een lokaal optimum: vermits enkel de beste oplossingen worden geaccepteerd in de populatie.
\end{itemize}
\subsection{\emph{AdapHH: Adaptive Hyper-heuristic} (\#1)}
\label{sss:adaphh}
\subsubsection{Implementatie}
AdapHH\cite{chesc-adaphh,chesc-adaphh2,348072} werkt met een populatie van \abhn{} die de \emph{Adaptive Dynamic Heuristic Set (ADHS)} wordt genoemd. Het is de bedoeling dat deze set heuristieken zich aanpast zodat telkens de op dat moment interessantste \abhn{} in de set zitten. Hiervoor deelt men de tijd op in fases. \abh[H]{} worden op het einde van een fase ge\"evalueerd als een gewogen som van vier metrieken: het aantal maal dat men een tot dan toe beste oplossing aanreikte, de totale fitness verbetering, de totale fitness verslechtering en de gespendeerde tijd. Bovendien maakt men een onderscheid tussen de bereikte resultaten in de fase en over de volledige termijn dat de hyperheuristiek draait: de gewichten van de effecten in de fase zijn wegen significant zwaarder door. Op het einde van een fase komt een deel van de \abhn{} in de ``\emph{Tabu List}''\cite{DBLP:journals/informs/Glover89}. Indien een \abh{} bij herintroductie onmiddellijk weer verwijdert wordt, wordt het aantal tabu-fases met \'e\'en verhoogt voor deze \abh{}. Indien deze teller een maximum bereikt wordt de heuristiek definitief geschrapt. Ook \abhn{} die tijdens de fase geen betere oplossing vonden, rust een tijdelijk tabu. Een tweede manier waarop men \abhn{} selecteert is de zogenaamde ``\emph{Relay Hybridisation}'': Elke \abhn{} houdt ook een lijstje van 10 andere metaheuristieken bij die na de deze \abh{} kunnen worden uitgevoerd. Dit systeem wordt vooral op het einde van een fase actief. Bij de \emph{relay hybridisation} worden dus twee \abhn{} na elkaar uitgevoerd. Een ander belangrijk component in het systeem is \emph{move acceptance}: het al dan niet beschouwen van het resultaat van de \abh{} als de nieuwe oplossing. Dit systeem wordt het \emph{Adaptive Iteration Limited List-based Threshold Accepting (AILLA) system} genoemd. Betere oplossingen worden altijd geaccepteerd. In het geval we een slechtere oplossing optekenen zullen we doorgaans weigeren, tenzij de oplossing beter is dan deze beste oplossing van enkele fases terug. Het aantal fases dat we terug mogen kijken wordt systematisch opgehoogd wanneer we een bepaalde beweging niet accepteren. Parameters ten slotte worden aangepast door de \abhn{} te categoriseren en op basis hiervan de \emph{Intensity of Mutation} of de \emph{Depth of Search} in kleine stappen te verhogen of te verlagen.

\section{Conclusies}
Het trekken van algemene conclusies bij de hierboven beschreven systemen is complex: de meeste systemen omvatten verschillende componenten en daarom kunnen we nooit eenduidig besluiten of de superioriteit van \'e\'en \abhh{} tegenover de andere wel degelijk te wijten is aan een bepaald aspect van deze \abhh{}.

\subsection{Tabu lijsten versus probabiliteitsvectoren}
Toch kunnen we op z'n minst enkele hypotheses naar voren schuiven. Een eerste hypothese is dat bij de selectie van \abllhn{} een aanpak met behulp van \emph{Tabu Search} doorgaans superieur lijkt te zijn aan de selectie door middel van probabiliteitsvectoren. Een waarschijnlijke verklaring is dat de probabiliteitsvectoren meestal in een onstabiele situatie verzeilt kunnen raken waarin omdat een bepaalde heuristiek vaker wordt geselecteerd, er meer kans is om de probabiliteit verder te verhogen. Hierdoor komen we in een situatie waarin na verloop van tijd slechts een gedeelte van de \abllhn{} nog regelmatig wordt opgeroepen. Deze \abllhn{} kunnen na verloop van tijd \abieff{} gedrag vertonen (we denken hierbij bijvoorbeeld aan het idempotent gedrag van \abls{}), maar worden hiervoor niet bestraft (en zelfs beloond). \emph{Tabu Search} kunnen we zien als een probabiliteitsvector waarbij enkele componenten kansloos zijn en de actieve \abhn{} de kansen eerlijk verdelen. Een belangrijk aspect van \emph{Tabu Search} is echter het \emph{forgive-and-forget}-karakter: na een zekere tijd komt een \abh{} weer in de actieve set en krijgt deze opnieuw een eerlijke kans. In het geval van \emph{AdapHH} zien we dat dit bovendien niet hoeft te betekenen dat we alle opgedane ervaring weggooien: de metriek die bepaalt welke \abhn{} er de volgende fase tabu worden bevat immers eigenschappen die een uitspraak doen over de volledige periode dat het algoritme actief is. Men geeft echter wel een doorslaggevende rol aan de laatste fase waardoor een \abh{} de kans krijgt om zich in een fase wel degelijk te bewijzen.

\subsection{Normalisatie van fitness-waardes}
Een ander belangrijk aspect is het rekenen met de fitness-waarde van een zekere oplossing. Men stelt vast dat de fitness-waarde dikwijls volgens een exponenti\"ele functie daalt. Dit betekent dat we in het begin een grote vooruitgang mogen verwachten en naar het einde toe een veel minder sterke vooruitgang. Dit is logisch vermits het in het begin makkelijk is om tot een betere oplossing te komen dan de voorheen gevonden oplossing. Na verloop van tijd zitten we echter vast op lokale optima die in de meeste gevallen in kwaliteit weinig verschillen met het globale optimum.
\paragraph{}
Het is van belang dat we rekening houden met dit fenomeen. We zien bijvoorbeeld dat slecht presterende \abhhn{} meestal een lineair beloningssysteem gebruiken: \abhn{} worden beloond op basis van het absolute of relatieve verschil in fitness-waarde tussen de oorspronkelijke en nieuwe oplossing. Dit betekent echter dat de \abhn{} die eerste geselecteerd worden meestal bevoordeeld worden tegenover later geselecteerde oplossingen die het meestal met minder vooruitgang moeten stellen.
\paragraph{}
We zien dat de betere \abhhn{} dit probleem op verschillende manieren oplossen. \emph{AdapHH} hecht bijvoorbeeld maar weinig belang aan de verschil in fitness-waarde: de belangrijkste metriek is het aantal keer dat de metaheuristiek een nieuw globaal optimum levert. De frequentie waarmee dit laatste gebeurt neemt ook af, maar minder sterk. Hierdoor geeft men meer gelegenheid aan een \abh{} om zich te bewijzen. Normalisatie zien we ook in de \emph{move acceptance} component: we accepteren een nieuwe oplossing op basis van de fitness-waarde van de meest recente globale optima.

\subsection{Behoud van status quo en tijdmeting}
Een ander aspect is het behouden van status-quo. In de kritieken kwam vaak aan bod dat het algoritme geen rekening houdt met de idempotentie van de \abls{} \abhn{}. Dit laatste leidt tot het verlies van een aanzienlijke hoeveelheid rekenkracht. Dit komt gedeeltelijk door de karakteristieken van \abls{}: bij een \abls{} \abh{} beschouwen we immers een relatief grote zoekruimte, voor elke instantie moet daarenboven de fitness-waarde uitgerekend worden. Mutatie-operatoren leveren per definitie een andere oplossing af en bestede hier meestal een minimum aan tijd aan.
\paragraph{}
Betere \abhhn{} bestraffen dan ook \abhn{} die een oplossing voorstellen die equivalent is aan het origineel. \emph{VNS-TW} bijvoorbeeld bestraft bijvoorbeeld in 20\% van de gevallen de \abh{} die tot een equivalent resultaat komt. \emph{AdapHH} gaat zelfs een stapje verder en bestraft onder bepaalde voorwaarden de \abhn{} die in een fase niet tot een nieuw globaal optimum kwamen en een significante hoeveelheid tijd in beslag namen.

\subsection{Robuuste parameters tijdens het zoekproces}
Heel wat algoritmes bevatten veel parameters. Sommige systemen ontwikkelen leersystemen die deze parameters moeten zetten. Een probleem met deze aanpak is dat deze leersystemen op hun beurt weer parameters introduceren. Meestal is er geen enkele garantie is dat deze parameters robuuster zullen zijn. Bij de meeste systemen ontbreekt een duidelijke motivatie die leersystemen voor parameters rechtvaardigt.
\paragraph{}
De meest succesvolle oplossing voor het oplossen van dit probleem bestaat er dan ook uit verschillende leersystemen tegelijk te beschouwen. Indien \'e\'en van de systemen dan faalt om op een effici\"ente manier tot een goede oplossing te komen is er nog altijd de hoop dat de andere systemen daar wel in slagen. Dit concept is bijvoorbeeld duidelijk ge\"implementeerd in \emph{AdapHH} waar naast een systeem met ``\emph{Tabu Search}'' ook het ``\emph{Hybrid Relay}'' systeem gebruikt wordt.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "masterproef"
%%% End: 
